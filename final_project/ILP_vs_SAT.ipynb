{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36d9b1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Treedepth ILP Integrated Experiment System\n",
      "============================================================\n",
      "Experiment start time: 2025-08-21 15:42:53\n",
      "Gurobi version: (12, 0, 3)\n",
      "Available CPU cores: 32\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import subprocess\n",
    "import importlib\n",
    "\n",
    "for pkg in (\"networkx\", \"gurobipy\", \"pandas\", \"matplotlib\", \"seaborn\"):\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "    except ImportError:\n",
    "        print(f\"Installing {pkg}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "\n",
    "import networkx as nx\n",
    "import gurobipy as gp \n",
    "from gurobipy import GRB\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Treedepth ILP Integrated Experiment System\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Experiment start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Gurobi version: {gp.gurobi.version()}\")\n",
    "print(f\"Available CPU cores: {os.cpu_count()}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5aa052e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment configuration:\n",
      "  TIMEOUT: 120\n",
      "  MAX_NODES: 50\n",
      "  MAX_EDGES: 2000\n",
      "  THREADS: 8\n",
      "  RANDOM_SEED: 42\n",
      "  MIPSTART_RUNS: 10\n",
      "\n",
      "Output files:\n",
      "  baseline: results_4_2_baseline_ilp.csv\n",
      "  preprocessing: results_4_3_preprocessing.csv\n",
      "  components: results_4_4_components.csv\n",
      "  variants: results_4_5_variants.csv\n",
      "  final_comparison: results_4_6_ilp_vs_sat.csv\n",
      "\n",
      "Skipped instances: 5\n",
      "CSV standard columns: 23\n",
      "\n",
      "✓ Experiment configuration complete\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENTAL_CONFIG = {\n",
    "    \"TIMEOUT\": 120,\n",
    "    \"MAX_NODES\": 50,\n",
    "    \"MAX_EDGES\": 2000,\n",
    "    \"THREADS\": max(1, os.cpu_count() // 4),\n",
    "    \"RANDOM_SEED\": 42,\n",
    "    \"MIPSTART_RUNS\": 10,\n",
    "}\n",
    "\n",
    "SKIP_LIST = {\"Paley17\", \"Holt\", \"Watsin\",\"McGee\",\"Kittell\"}\n",
    "\n",
    "DATASET_PATHS = {\n",
    "    \"famous\": \"inputs/famous/*.edge\",\n",
    "    \"standard\": \"inputs/standard/*.edge\"\n",
    "}\n",
    "\n",
    "OUTPUT_FILES = {\n",
    "    \"baseline\": \"results_4_2_baseline_ilp.csv\",\n",
    "    \"preprocessing\": \"results_4_3_preprocessing.csv\", \n",
    "    \"components\": \"results_4_4_components.csv\",\n",
    "    \"variants\": \"results_4_5_variants.csv\",\n",
    "    \"final_comparison\": \"results_4_6_ilp_vs_sat.csv\"\n",
    "}\n",
    "\n",
    "STANDARD_CSV_HEADERS = [\n",
    "    \"experiment\",\n",
    "    \"instance\",\n",
    "    \"dataset\",\n",
    "    \"n\",\n",
    "    \"m\",\n",
    "    \"treedepth\",\n",
    "    \"time\",\n",
    "    \"status\",\n",
    "    \"timeout\",\n",
    "    \"memory_usage_mb\",\n",
    "    \"preprocessing_enabled\",\n",
    "    \"mipstart_enabled\",\n",
    "    \"lazy_constraints_enabled\",\n",
    "    \"gurobi_presolve\",\n",
    "    \"encoding_variant\",\n",
    "    \"gurobi_objval\",\n",
    "    \"gurobi_mip_gap\",\n",
    "    \"gurobi_node_count\",\n",
    "    \"problem_variables\",\n",
    "    \"problem_constraints\",\n",
    "    \"graph_density\",\n",
    "    \"graph_components\",\n",
    "    \"preprocessing_reduction\",\n",
    "]\n",
    "\n",
    "print(\"Experiment configuration:\")\n",
    "for key, value in EXPERIMENTAL_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nOutput files:\")\n",
    "for exp, filename in OUTPUT_FILES.items():\n",
    "    print(f\"  {exp}: {filename}\")\n",
    "\n",
    "print(f\"\\nSkipped instances: {len(SKIP_LIST)}\")\n",
    "print(f\"CSV standard columns: {len(STANDARD_CSV_HEADERS)}\")\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "print(\"\\n✓ Experiment configuration complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37cc45cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Golden standard validation system loaded (timeout = error)\n",
      " Contains 35 instances with correct treedepth\n",
      " Validation rule: Only optimal solutions within time are correct\n",
      " Timeout upper bounds are errors\n",
      "\n",
      " Golden standard data:\n",
      "  Diamond: 3\n",
      "  Bull: 3\n",
      "  Butterfly: 3\n",
      "  Prism: 5\n",
      "  Moser: 5\n",
      "  Wagner: 6\n",
      "  Pmin: 5\n",
      "  3x3-grid: 5\n",
      "  Petersen: 6\n",
      "  Herschel: 5\n",
      "  ... total 35 instances\n",
      "\n",
      " Strict validation system ready (timeout = error)\n"
     ]
    }
   ],
   "source": [
    "GOLDEN_STANDARD = {\n",
    "    \"Diamond\": 3, \"Bull\": 3, \"Butterfly\": 3, \"Prism\": 5, \"Moser\": 5,\n",
    "    \"Wagner\": 6, \"Pmin\": 5, \"3x3-grid\": 5, \"Petersen\": 6, \"Herschel\": 5,\n",
    "    \"Grotzsch\": 7, \"Goldner\": 5, \"Durer\": 7, \"Franklin\": 7, \"Frucht\": 6,\n",
    "    \"Tietze\": 7, \"Chvatal\": 8, \"Paley13\": 10, \"Poussin\": 9, \"4x4-grid\": 7,\n",
    "    \"Sousselier\": 8, \"Hoffman\": 8, \"Clebsch\": 10, \"Shrikhande\": 11, \"Errera\": 10,\n",
    "    \"Pappus\": 8, \"Robertson\": 10, \"Desargues\": 9, \"Dodecahedron\": 9, \n",
    "    \"FlowerSnark\": 9, \"Folkman\": 9, \"Brinkmann\": 11, \"Kittell\": 12, \n",
    "    \"McGee\": 11, \"Nauru\": 10\n",
    "}\n",
    "\n",
    "def validate_result(instance_name: str, computed_value: Optional[int], is_timeout: bool = False) -> Dict[str, Any]:\n",
    "    validation_result = {\n",
    "        \"instance\": instance_name,\n",
    "        \"golden_value\": GOLDEN_STANDARD.get(instance_name),\n",
    "        \"computed_value\": computed_value,\n",
    "        \"is_timeout\": is_timeout,\n",
    "        \"is_correct\": False,\n",
    "        \"error_type\": None,\n",
    "        \"validation_status\": \"unknown\"\n",
    "    }\n",
    "    \n",
    "    if instance_name not in GOLDEN_STANDARD:\n",
    "        validation_result[\"validation_status\"] = \"no_golden_standard\"\n",
    "        validation_result[\"is_correct\"] = None\n",
    "        return validation_result\n",
    "    \n",
    "    golden_value = GOLDEN_STANDARD[instance_name]\n",
    "    \n",
    "    if computed_value is None:\n",
    "        validation_result[\"error_type\"] = \"failed_to_solve\"\n",
    "        validation_result[\"validation_status\"] = \"failed\"\n",
    "        return validation_result\n",
    "    \n",
    "    if is_timeout:\n",
    "        validation_result[\"is_correct\"] = False\n",
    "        validation_result[\"error_type\"] = \"timeout_not_optimal\"\n",
    "        validation_result[\"validation_status\"] = \"error\"\n",
    "        return validation_result\n",
    "    \n",
    "    if computed_value == golden_value:\n",
    "        validation_result[\"is_correct\"] = True\n",
    "        validation_result[\"validation_status\"] = \"correct_optimal\"\n",
    "    else:\n",
    "        validation_result[\"is_correct\"] = False\n",
    "        if computed_value < golden_value:\n",
    "            validation_result[\"error_type\"] = \"underestimate\"\n",
    "        else:\n",
    "            validation_result[\"error_type\"] = \"overestimate\"\n",
    "        validation_result[\"validation_status\"] = \"error\"\n",
    "    \n",
    "    return validation_result\n",
    "\n",
    "def analyze_validation_results(validation_results: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    total = len(validation_results)\n",
    "    if total == 0:\n",
    "        return {}\n",
    "    \n",
    "    testable_results = [r for r in validation_results if r[\"validation_status\"] != \"no_golden_standard\"]\n",
    "    testable_count = len(testable_results)\n",
    "    \n",
    "    if testable_count == 0:\n",
    "        return {\"total_instances\": total, \"testable_instances\": 0, \"accuracy_rate\": 0}\n",
    "    \n",
    "    correct_count = len([r for r in testable_results if r[\"is_correct\"]])\n",
    "    error_count = len([r for r in testable_results if r[\"validation_status\"] == \"error\"])\n",
    "    failed_count = len([r for r in testable_results if r[\"validation_status\"] == \"failed\"])\n",
    "    timeout_errors = len([r for r in testable_results if r[\"error_type\"] == \"timeout_not_optimal\"])\n",
    "    \n",
    "    error_types = {}\n",
    "    for result in testable_results:\n",
    "        if result[\"error_type\"]:\n",
    "            error_types[result[\"error_type\"]] = error_types.get(result[\"error_type\"], 0) + 1\n",
    "    \n",
    "    analysis = {\n",
    "        \"total_instances\": total,\n",
    "        \"testable_instances\": testable_count,\n",
    "        \"correct_count\": correct_count,\n",
    "        \"error_count\": error_count,\n",
    "        \"failed_count\": failed_count,\n",
    "        \"timeout_errors\": timeout_errors,\n",
    "        \"accuracy_rate\": correct_count / testable_count,\n",
    "        \"error_rate\": error_count / testable_count,\n",
    "        \"failure_rate\": failed_count / testable_count,\n",
    "        \"timeout_error_rate\": timeout_errors / testable_count,\n",
    "        \"error_types\": error_types\n",
    "    }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def calculate_method_metrics(results: List[Dict], method_name: str = \"\"):\n",
    "    if not results:\n",
    "        return {\n",
    "            \"method_name\": method_name,\n",
    "            \"accuracy_rate\": 0,\n",
    "            \"success_rate\": 0,\n",
    "            \"optimal_rate\": 0,\n",
    "            \"avg_time\": float('inf'),\n",
    "            \"testable_instances\": 0,\n",
    "            \"correct_instances\": 0,\n",
    "            \"timeout_instances\": 0,\n",
    "            \"is_valid\": False\n",
    "        }\n",
    "    \n",
    "    validation_results = []\n",
    "    for r in results:\n",
    "        validation = validate_result(r.get(\"instance\", \"\"), r.get(\"treedepth\"), r.get(\"timeout\", False))\n",
    "        validation_results.append(validation)\n",
    "    \n",
    "    analysis = analyze_validation_results(validation_results)\n",
    "    accuracy_rate = analysis.get(\"accuracy_rate\", 0)\n",
    "    testable_instances = analysis.get(\"testable_instances\", 0)\n",
    "    correct_instances = analysis.get(\"correct_count\", 0)\n",
    "    \n",
    "    success_results = [r for r in results if r.get(\"status\") in [\"optimal\", \"timeout\"]]\n",
    "    success_rate = len(success_results) / len(results) if results else 0\n",
    "    \n",
    "    optimal_results = [r for r in results if r.get(\"status\") == \"optimal\"]\n",
    "    optimal_rate = len(optimal_results) / len(results) if results else 0\n",
    "    \n",
    "    timeout_results = [r for r in results if r.get(\"status\") == \"timeout\"]\n",
    "    timeout_instances = len(timeout_results)\n",
    "    \n",
    "    successful_times = [r.get(\"time\", float('inf')) for r in success_results if \"time\" in r and r[\"time\"] is not None]\n",
    "    avg_time = np.mean(successful_times) if successful_times else float('inf')\n",
    "    \n",
    "    return {\n",
    "        \"method_name\": method_name,\n",
    "        \"accuracy_rate\": accuracy_rate,\n",
    "        \"success_rate\": success_rate,\n",
    "        \"optimal_rate\": optimal_rate,\n",
    "        \"avg_time\": avg_time,\n",
    "        \"testable_instances\": testable_instances,\n",
    "        \"correct_instances\": correct_instances,\n",
    "        \"timeout_instances\": timeout_instances,\n",
    "        \"error_instances\": analysis.get(\"error_count\", 0),\n",
    "        \"failed_instances\": analysis.get(\"failed_count\", 0),\n",
    "        \"error_types\": analysis.get(\"error_types\", {}),\n",
    "        \"is_valid\": testable_instances > 0\n",
    "    }\n",
    "\n",
    "def select_best_method_tiered(methods_metrics: List[Dict], accuracy_tolerance: float = 0.02):\n",
    "    if not methods_metrics:\n",
    "        return None\n",
    "    \n",
    "    valid_methods = [m for m in methods_metrics if m[\"is_valid\"]]\n",
    "    \n",
    "    if not valid_methods:\n",
    "        print(\"⚠️ Warning: No method has testable instances\")\n",
    "        return None\n",
    "    \n",
    "    valid_methods.sort(key=lambda x: x[\"accuracy_rate\"], reverse=True)\n",
    "    best_accuracy = valid_methods[0][\"accuracy_rate\"]\n",
    "    \n",
    "    top_accuracy_methods = [m for m in valid_methods \n",
    "                           if abs(m[\"accuracy_rate\"] - best_accuracy) <= accuracy_tolerance]\n",
    "    \n",
    "    print(f\"\\n🎯 First round (Accuracy - only optimal within timeout):\")\n",
    "    print(f\"   Best accuracy: {best_accuracy:.1%}\")\n",
    "    print(f\"   Methods within tolerance: {len(top_accuracy_methods)}\")\n",
    "    for m in top_accuracy_methods:\n",
    "        print(f\"     {m['method_name']}: Accuracy {m['accuracy_rate']:.1%}, Optimal rate {m['optimal_rate']:.1%}\")\n",
    "    \n",
    "    if len(top_accuracy_methods) == 1:\n",
    "        best_method = top_accuracy_methods[0]\n",
    "        print(f\"\\n Selection: {best_method['method_name']} (unique highest accuracy)\")\n",
    "        return best_method\n",
    "    \n",
    "    print(f\"\\n Second round (Optimal rate):\")\n",
    "    top_accuracy_methods.sort(key=lambda x: x[\"optimal_rate\"], reverse=True)\n",
    "    \n",
    "    best_optimal_rate = top_accuracy_methods[0][\"optimal_rate\"]\n",
    "    best_optimal_candidates = [m for m in top_accuracy_methods \n",
    "                              if abs(m[\"optimal_rate\"] - best_optimal_rate) <= 0.02]\n",
    "    \n",
    "    for m in best_optimal_candidates[:3]:\n",
    "        time_str = f\"{m['avg_time']:.2f}s\" if m['avg_time'] != float('inf') else \"∞\"\n",
    "        print(f\"     {m['method_name']}: Optimal rate {m['optimal_rate']:.1%}, Avg time {time_str}\")\n",
    "    \n",
    "    best_method = min(best_optimal_candidates, key=lambda x: x[\"avg_time\"])\n",
    "    \n",
    "    print(f\"\\n🏆 Final selection: {best_method['method_name']}\")\n",
    "    print(f\"   Accuracy: {best_method['accuracy_rate']:.1%} (only optimal within timeout)\")\n",
    "    print(f\"   Optimal rate: {best_method['optimal_rate']:.1%}\")\n",
    "    time_str = f\"{best_method['avg_time']:.2f}s\" if best_method['avg_time'] != float('inf') else \"∞\"\n",
    "    print(f\"   Avg time: {time_str}\")\n",
    "    \n",
    "    return best_method\n",
    "\n",
    "def display_validation_summary(validation_results: List[Dict[str, Any]], method_name: str = \"\"):\n",
    "    analysis = analyze_validation_results(validation_results)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{method_name} Validation Results (timeout = error)\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total instances: {analysis['total_instances']}\")\n",
    "    print(f\"Testable instances: {analysis['testable_instances']} (with golden standard)\")\n",
    "    print(f\" Correct: {analysis['correct_count']} (only optimal within timeout)\")\n",
    "    print(f\" Errors: {analysis['error_count']}\")\n",
    "    print(f\"    - Timeout errors: {analysis['timeout_errors']}\")\n",
    "    print(f\" Failed: {analysis['failed_count']}\")\n",
    "    print(f\" Accuracy: {analysis['accuracy_rate']:.1%} (only optimal within timeout)\")\n",
    "    \n",
    "    if analysis['error_types']:\n",
    "        print(f\"\\nError types:\")\n",
    "        for error_type, count in analysis['error_types'].items():\n",
    "            error_desc = {\n",
    "                \"timeout_not_optimal\": \"Timeout (upper bound not valid)\",\n",
    "                \"underestimate\": \"Underestimate\", \n",
    "                \"overestimate\": \"Overestimate\",\n",
    "                \"failed_to_solve\": \"Failed to solve\"\n",
    "            }\n",
    "            desc = error_desc.get(error_type, error_type)\n",
    "            print(f\"  {desc}: {count}\")\n",
    "    \n",
    "    error_instances = [r for r in validation_results if r[\"validation_status\"] == \"error\"]\n",
    "    if error_instances:\n",
    "        print(f\"\\n Error details:\")\n",
    "        for err in error_instances[:5]:\n",
    "            if err[\"error_type\"] == \"timeout_not_optimal\":\n",
    "                print(f\"  {err['instance']}: Timeout upper bound {err['computed_value']} (True: {err['golden_value']})\")\n",
    "            else:\n",
    "                print(f\"  {err['instance']}: Computed={err['computed_value']}, True={err['golden_value']}, Error={err['error_type']}\")\n",
    "        if len(error_instances) > 5:\n",
    "            print(f\"  ... {len(error_instances)-5} more errors\")\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "print(\" Golden standard validation system loaded (timeout = error)\")\n",
    "print(f\" Contains {len(GOLDEN_STANDARD)} instances with correct treedepth\")\n",
    "print(\" Validation rule: Only optimal solutions within time are correct\")\n",
    "print(\" Timeout upper bounds are errors\")\n",
    "\n",
    "print(f\"\\n Golden standard data:\")\n",
    "for i, (name, value) in enumerate(list(GOLDEN_STANDARD.items())[:10]):\n",
    "    print(f\"  {name}: {value}\")\n",
    "if len(GOLDEN_STANDARD) > 10:\n",
    "    print(f\"  ... total {len(GOLDEN_STANDARD)} instances\")\n",
    "\n",
    "print(\"\\n Strict validation system ready (timeout = error)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2bcc79cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing utility functions\n",
      "Found 40 instance files\n",
      "Loaded: 33 valid instances\n",
      "  Small (<=10 nodes): 9\n",
      "  Medium (11-20 nodes): 22\n",
      "  Large (>20 nodes): 2\n",
      "Loaded 33 test instances\n",
      "\n",
      "First 5 instances stats:\n",
      "  Brinkmann: n=21, m=42, density=0.200, estimated_bounds=[2, 21]\n",
      "  Bull: n=5, m=5, density=0.500, estimated_bounds=[2, 4]\n",
      "  Butterfly: n=5, m=6, density=0.600, estimated_bounds=[2, 3]\n",
      "  Chvatal: n=12, m=24, density=0.364, estimated_bounds=[2, 12]\n",
      "  Clebsch: n=16, m=40, density=0.333, estimated_bounds=[2, 16]\n"
     ]
    }
   ],
   "source": [
    "def read_edge_file(filename: str) -> Tuple[List[Tuple[int, int]], int]:\n",
    "    edges = []\n",
    "    n_declared = 0\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line or line.startswith('c'):\n",
    "                    continue\n",
    "                if line.startswith('p '):\n",
    "                    parts = line.split()\n",
    "                    if len(parts) >= 3:\n",
    "                        n_declared = int(parts[2])\n",
    "                elif line.startswith('e '):\n",
    "                    parts = line.split()\n",
    "                    if len(parts) >= 3:\n",
    "                        u, v = int(parts[1]) - 1, int(parts[2]) - 1\n",
    "                        edges.append((u, v))\n",
    "        return edges, n_declared\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to read file: {e}\")\n",
    "\n",
    "def create_graph_from_edges(edges: List[Tuple[int, int]], n_nodes: int) -> nx.Graph:\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(n_nodes))\n",
    "    G.add_edges_from(edges)\n",
    "    return G\n",
    "\n",
    "def get_graph_statistics(G: nx.Graph) -> Dict[str, Any]:\n",
    "    n = G.number_of_nodes()\n",
    "    m = G.number_of_edges()\n",
    "    return {\n",
    "        \"n\": n,\n",
    "        \"m\": m,\n",
    "        \"density\": m / (n * (n - 1) / 2) if n > 1 else 0,\n",
    "        \"components\": nx.number_connected_components(G),\n",
    "        \"is_connected\": nx.is_connected(G),\n",
    "        \"avg_degree\": 2 * m / n if n > 0 else 0,\n",
    "        \"max_degree\": max(dict(G.degree()).values()) if n > 0 else 0\n",
    "    }\n",
    "\n",
    "def load_test_instances() -> List[Tuple[str, str, nx.Graph, Dict[str, Any]]]:\n",
    "    instances = []\n",
    "    files = []\n",
    "    for dataset, pattern in DATASET_PATHS.items():\n",
    "        files.extend([(f, dataset) for f in glob.glob(pattern)])\n",
    "    files.sort(key=lambda x: x[0])\n",
    "    print(f\"Found {len(files)} instance files\")\n",
    "    for filepath, dataset in files:\n",
    "        instance = os.path.splitext(os.path.basename(filepath))[0]\n",
    "        if instance in SKIP_LIST:\n",
    "            continue\n",
    "        try:\n",
    "            edges, n_declared = read_edge_file(filepath)\n",
    "            G = create_graph_from_edges(edges, n_declared)\n",
    "            stats = get_graph_statistics(G)\n",
    "            if stats[\"n\"] > EXPERIMENTAL_CONFIG[\"MAX_NODES\"] or stats[\"m\"] > EXPERIMENTAL_CONFIG[\"MAX_EDGES\"]:\n",
    "                continue\n",
    "            instances.append((instance, dataset, G, stats))\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: skip instance {instance}: {e}\")\n",
    "            continue\n",
    "    print(f\"Loaded: {len(instances)} valid instances\")\n",
    "    small = len([i for i in instances if i[3][\"n\"] <= 10])\n",
    "    medium = len([i for i in instances if 10 < i[3][\"n\"] <= 20])\n",
    "    large = len([i for i in instances if i[3][\"n\"] > 20])\n",
    "    print(f\"  Small (<=10 nodes): {small}\")\n",
    "    print(f\"  Medium (11-20 nodes): {medium}\")\n",
    "    print(f\"  Large (>20 nodes): {large}\")\n",
    "    return instances\n",
    "\n",
    "def estimate_bounds(G: nx.Graph) -> Tuple[int, int]:\n",
    "    n = G.number_of_nodes()\n",
    "    if n <= 1:\n",
    "        return n, n\n",
    "    LB = 1\n",
    "    for comp in nx.connected_components(G):\n",
    "        H = G.subgraph(comp)\n",
    "        if H.number_of_nodes() == 1:\n",
    "            LBc = 1\n",
    "        else:\n",
    "            src = next(iter(comp))\n",
    "            dist1 = nx.single_source_shortest_path_length(G, src)\n",
    "            far = max(dist1, key=dist1.get)\n",
    "            dist2 = nx.single_source_shortest_path_length(G, far)\n",
    "            diam = max(dist2.values())\n",
    "            LBc = max(1, math.ceil(math.log2(diam + 1)))\n",
    "        LB = max(LB, LBc)\n",
    "    def dfs_height(component_nodes: List[int]) -> int:\n",
    "        visited = set()\n",
    "        maxh = 1\n",
    "        for start in component_nodes:\n",
    "            if start in visited:\n",
    "                continue\n",
    "            stack = [(start, 1, None)]\n",
    "            while stack:\n",
    "                v, d, parent = stack.pop()\n",
    "                if v in visited:\n",
    "                    continue\n",
    "                visited.add(v)\n",
    "                maxh = max(maxh, d)\n",
    "                for w in G.neighbors(v):\n",
    "                    if w != parent:\n",
    "                        stack.append((w, d + 1, v))\n",
    "        return maxh\n",
    "    UB = 0\n",
    "    for comp in nx.connected_components(G):\n",
    "        UB = max(UB, dfs_height(list(comp)))\n",
    "    UB = min(UB, n)\n",
    "    return LB, UB\n",
    "\n",
    "print(\"Testing utility functions\")\n",
    "test_instances = load_test_instances()\n",
    "print(f\"Loaded {len(test_instances)} test instances\")\n",
    "\n",
    "print(\"\\nFirst 5 instances stats:\")\n",
    "for i, (name, dataset, G, stats) in enumerate(test_instances[:5]):\n",
    "    lb, ub = estimate_bounds(G)\n",
    "    print(f\"  {name}: n={stats['n']}, m={stats['m']}, density={stats['density']:.3f}, estimated_bounds=[{lb}, {ub}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "513371fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing preprocessing functions...\n",
      "\n",
      "Demo instance: Chvatal\n",
      "  Original: n=12, m=24\n",
      "  Without preprocessing: n=12, m=24\n",
      "  With preprocessing: n=12, m=24\n",
      "  Degree-one removed: 0\n",
      "  Apex removed: 0\n",
      "  Reduction rate: 0.0%\n",
      "Analyzing preprocessing effectiveness...\n",
      "Preprocessing analysis result:\n",
      "  Total instances: 33\n",
      "  Effective preprocessing: 2 (6.1%)\n",
      "  Average node reduction rate: 0.021\n",
      "\n",
      "Instances with more than 10% node reduction:\n",
      "  Diamond: 4→2 (reduction 50.0%)\n",
      "  Butterfly: 5→4 (reduction 20.0%)\n",
      "\n",
      " Preprocessing functions test complete\n"
     ]
    }
   ],
   "source": [
    "def apex_vertices(G: nx.Graph) -> Tuple[nx.Graph, int]:\n",
    "    if G.number_of_nodes() <= 1:\n",
    "        return G.copy(), 0\n",
    "    n = G.number_of_nodes()\n",
    "    to_remove = [v for v, degree in G.degree() if degree == n - 1]\n",
    "    G_processed = G.copy()\n",
    "    G_processed.remove_nodes_from(to_remove)\n",
    "    G_processed = nx.convert_node_labels_to_integers(G_processed, first_label=0, ordering=\"default\")\n",
    "    return G_processed, len(to_remove)\n",
    "\n",
    "def degree_one_reduction(G: nx.Graph) -> nx.Graph:\n",
    "    if G.number_of_nodes() <= 1:\n",
    "        return G.copy()\n",
    "    G_processed = G.copy()\n",
    "    to_remove = set()\n",
    "    for u in G_processed.nodes():\n",
    "        degree_one_neighbors = [v for v in G_processed.neighbors(u) if G_processed.degree(v) == 1]\n",
    "        if len(degree_one_neighbors) > 1:\n",
    "            to_remove.update(degree_one_neighbors[1:])\n",
    "    G_processed.remove_nodes_from(to_remove)\n",
    "    G_processed = nx.convert_node_labels_to_integers(G_processed, first_label=0, ordering=\"default\")\n",
    "    return G_processed\n",
    "\n",
    "def preprocess_graph(G: nx.Graph, enable_preprocessing: bool = True) -> Tuple[nx.Graph, int, Dict[str, Any]]:\n",
    "    original_stats = get_graph_statistics(G)\n",
    "    preprocess_stats = {\n",
    "        \"original_nodes\": original_stats[\"n\"],\n",
    "        \"original_edges\": original_stats[\"m\"],\n",
    "        \"degree_one_removed\": 0,\n",
    "        \"apex_removed\": 0,\n",
    "        \"final_nodes\": 0,\n",
    "        \"final_edges\": 0,\n",
    "        \"reduction_rate\": 0.0,\n",
    "        \"preprocessing_enabled\": enable_preprocessing\n",
    "    }\n",
    "    if not enable_preprocessing:\n",
    "        preprocess_stats.update({\n",
    "            \"final_nodes\": original_stats[\"n\"],\n",
    "            \"final_edges\": original_stats[\"m\"]\n",
    "        })\n",
    "        return G.copy(), 0, preprocess_stats\n",
    "    G_step1 = degree_one_reduction(G)\n",
    "    degree_one_removed = original_stats[\"n\"] - G_step1.number_of_nodes()\n",
    "    G_step2, apex_removed = apex_vertices(G_step1)\n",
    "    final_stats = get_graph_statistics(G_step2)\n",
    "    reduction_rate = 1.0 - (final_stats[\"n\"] / original_stats[\"n\"]) if original_stats[\"n\"] > 0 else 0.0\n",
    "    preprocess_stats.update({\n",
    "        \"degree_one_removed\": degree_one_removed,\n",
    "        \"apex_removed\": apex_removed,\n",
    "        \"final_nodes\": final_stats[\"n\"],\n",
    "        \"final_edges\": final_stats[\"m\"],\n",
    "        \"reduction_rate\": reduction_rate\n",
    "    })\n",
    "    return G_step2, apex_removed, preprocess_stats\n",
    "\n",
    "def analyze_preprocessing_effectiveness():\n",
    "    print(\"Analyzing preprocessing effectiveness...\")\n",
    "    preprocessing_analysis = []\n",
    "    for instance, dataset, G, original_stats in test_instances:\n",
    "        G_no_prep, apex_buf_no, stats_no = preprocess_graph(G, enable_preprocessing=False)\n",
    "        G_prep, apex_buf_yes, stats_yes = preprocess_graph(G, enable_preprocessing=True)\n",
    "        analysis_record = {\n",
    "            \"instance\": instance,\n",
    "            \"dataset\": dataset,\n",
    "            \"original_n\": original_stats[\"n\"],\n",
    "            \"original_m\": original_stats[\"m\"],\n",
    "            \"no_prep_n\": stats_no[\"final_nodes\"],\n",
    "            \"no_prep_m\": stats_no[\"final_edges\"],\n",
    "            \"prep_n\": stats_yes[\"final_nodes\"],\n",
    "            \"prep_m\": stats_yes[\"final_edges\"],\n",
    "            \"degree_one_removed\": stats_yes[\"degree_one_removed\"],\n",
    "            \"apex_removed\": stats_yes[\"apex_removed\"],\n",
    "            \"reduction_rate\": stats_yes[\"reduction_rate\"],\n",
    "            \"effective\": stats_yes[\"reduction_rate\"] > 0.0\n",
    "        }\n",
    "        preprocessing_analysis.append(analysis_record)\n",
    "    effective_count = sum(1 for r in preprocessing_analysis if r[\"effective\"])\n",
    "    total_instances = len(preprocessing_analysis)\n",
    "    avg_reduction = np.mean([r[\"reduction_rate\"] for r in preprocessing_analysis])\n",
    "    print(\"Preprocessing analysis result:\")\n",
    "    print(f\"  Total instances: {total_instances}\")\n",
    "    print(f\"  Effective preprocessing: {effective_count} ({effective_count/total_instances*100:.1f}%)\")\n",
    "    print(f\"  Average node reduction rate: {avg_reduction:.3f}\")\n",
    "    effective_cases = [r for r in preprocessing_analysis if r[\"reduction_rate\"] > 0.1]\n",
    "    if effective_cases:\n",
    "        print(\"\\nInstances with more than 10% node reduction:\")\n",
    "        for case in sorted(effective_cases, key=lambda x: x[\"reduction_rate\"], reverse=True)[:5]:\n",
    "            print(f\"  {case['instance']}: {case['original_n']}→{case['prep_n']} (reduction {case['reduction_rate']:.1%})\")\n",
    "    return preprocessing_analysis\n",
    "\n",
    "print(\"Testing preprocessing functions...\")\n",
    "\n",
    "demo_instance = None\n",
    "for instance, dataset, G, stats in test_instances:\n",
    "    if 8 <= stats[\"n\"] <= 15:\n",
    "        demo_instance = (instance, dataset, G, stats)\n",
    "        break\n",
    "\n",
    "if demo_instance:\n",
    "    name, dataset, G, stats = demo_instance\n",
    "    print(f\"\\nDemo instance: {name}\")\n",
    "    print(f\"  Original: n={stats['n']}, m={stats['m']}\")\n",
    "    G_no, apex_no, stats_no = preprocess_graph(G, enable_preprocessing=False)\n",
    "    print(f\"  Without preprocessing: n={stats_no['final_nodes']}, m={stats_no['final_edges']}\")\n",
    "    G_yes, apex_yes, stats_yes = preprocess_graph(G, enable_preprocessing=True)\n",
    "    print(f\"  With preprocessing: n={stats_yes['final_nodes']}, m={stats_yes['final_edges']}\")\n",
    "    print(f\"  Degree-one removed: {stats_yes['degree_one_removed']}\")\n",
    "    print(f\"  Apex removed: {stats_yes['apex_removed']}\")\n",
    "    print(f\"  Reduction rate: {stats_yes['reduction_rate']:.1%}\")\n",
    "\n",
    "preprocessing_effectiveness = analyze_preprocessing_effectiveness()\n",
    "\n",
    "print(\"\\n Preprocessing functions test complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57df806d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MIPStart generation...\n",
      "\n",
      "Test instance: Brinkmann (n=21, m=42)\n",
      "  Deterministic DFS height: 20\n",
      "  Randomized DFS height: 20\n",
      "  Best height from multiple randomized runs: 18\n",
      "  Solution check: depth range=[1, 20], number of roots=1\n",
      "\n",
      "Testing solver configuration...\n",
      "  Base config: MIP=True, Lazy=True, Presolve=2\n",
      "  No MIPStart: MIP=False, Lazy=True, Presolve=2\n",
      "  No Lazy constraints: MIP=True, Lazy=False, Presolve=2\n",
      "  No Presolve: MIP=True, Lazy=True, Presolve=0\n",
      "\n",
      "MIPStart and solver configuration tests complete\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def build_deterministic_dfs_mipstart(G: nx.Graph) -> Dict[str, Any]:\n",
    "    nodes = list(G.nodes())\n",
    "    if not nodes:\n",
    "        return {\"H\": 0, \"depth\": {}, \"parent\": {}, \"ancestor\": {}, \"root\": {}}\n",
    "    parent = {v: None for v in nodes}\n",
    "    depth = {v: None for v in nodes}\n",
    "    def dfs(root: int):\n",
    "        stack = [(root, 1, None)]\n",
    "        while stack:\n",
    "            v, d, p = stack.pop()\n",
    "            if depth[v] is not None:\n",
    "                continue\n",
    "            depth[v] = d\n",
    "            parent[v] = p\n",
    "            neighbors = sorted(G.neighbors(v))\n",
    "            for w in reversed(neighbors):\n",
    "                if depth[w] is None:\n",
    "                    stack.append((w, d + 1, v))\n",
    "    for comp in nx.connected_components(G):\n",
    "        root = min(comp)\n",
    "        dfs(root)\n",
    "    H = max(depth.values()) if depth else 0\n",
    "    def is_ancestor(u, v):\n",
    "        cur = parent[v]\n",
    "        while cur is not None:\n",
    "            if cur == u:\n",
    "                return True\n",
    "            cur = parent[cur]\n",
    "        return False\n",
    "    ancestor = {(u, v): int(u != v and is_ancestor(u, v)) for u in nodes for v in nodes}\n",
    "    root = {v: int(parent[v] is None) for v in nodes}\n",
    "    return {\"H\": H, \"depth\": depth, \"parent\": parent, \"ancestor\": ancestor, \"root\": root}\n",
    "\n",
    "def build_randomized_dfs_mipstart(G: nx.Graph, seed: Optional[int] = None) -> Dict[str, Any]:\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    nodes = list(G.nodes())\n",
    "    if not nodes:\n",
    "        return {\"H\": 0, \"depth\": {}, \"parent\": {}, \"ancestor\": {}, \"root\": {}}\n",
    "    parent = {v: None for v in nodes}\n",
    "    depth = {v: None for v in nodes}\n",
    "    def randomized_dfs(root: int):\n",
    "        stack = [(root, 1, None)]\n",
    "        while stack:\n",
    "            v, d, p = stack.pop()\n",
    "            if depth[v] is not None:\n",
    "                continue\n",
    "            depth[v] = d\n",
    "            parent[v] = p\n",
    "            neighbors = list(G.neighbors(v))\n",
    "            random.shuffle(neighbors)\n",
    "            for w in neighbors:\n",
    "                if depth[w] is None:\n",
    "                    stack.append((w, d + 1, v))\n",
    "    for comp in nx.connected_components(G):\n",
    "        comp_list = list(comp)\n",
    "        root = random.choice(comp_list)\n",
    "        randomized_dfs(root)\n",
    "    H = max(depth.values()) if depth else 0\n",
    "    def is_ancestor(u, v):\n",
    "        cur = parent[v]\n",
    "        while cur is not None:\n",
    "            if cur == u:\n",
    "                return True\n",
    "            cur = parent[cur]\n",
    "        return False\n",
    "    ancestor = {(u, v): int(u != v and is_ancestor(u, v)) for u in nodes for v in nodes}\n",
    "    root = {v: int(parent[v] is None) for v in nodes}\n",
    "    return {\"H\": H, \"depth\": depth, \"parent\": parent, \"ancestor\": ancestor, \"root\": root}\n",
    "\n",
    "def build_multiple_mipstarts(G: nx.Graph, num_runs: int = 10) -> Dict[str, Any]:\n",
    "    best_solution = None\n",
    "    best_height = float('inf')\n",
    "    for run in range(num_runs):\n",
    "        try:\n",
    "            solution = build_randomized_dfs_mipstart(G, seed=run)\n",
    "            height = solution[\"H\"]\n",
    "            if height < best_height:\n",
    "                best_height = height\n",
    "                best_solution = solution\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    if best_solution is None:\n",
    "        best_solution = build_deterministic_dfs_mipstart(G)\n",
    "    return best_solution\n",
    "\n",
    "def setup_gurobi_model(model: gp.Model, component_config: Dict[str, Any]):\n",
    "    model.Params.OutputFlag = 0\n",
    "    model.Params.TimeLimit = EXPERIMENTAL_CONFIG[\"TIMEOUT\"]\n",
    "    model.Params.Threads = EXPERIMENTAL_CONFIG[\"THREADS\"]\n",
    "    model.Params.Cuts = 2\n",
    "    model.Params.Symmetry = 2\n",
    "    model.Params.NodefileStart = 0.5\n",
    "    if component_config.get(\"mipstart_enabled\", True):\n",
    "        model.Params.MIPFocus = 1\n",
    "        model.Params.Heuristics = 0.4\n",
    "    else:\n",
    "        model.Params.StartNumber = 0\n",
    "        model.Params.Heuristics = 0.1\n",
    "    presolve_level = component_config.get(\"gurobi_presolve\", 2)\n",
    "    model.Params.Presolve = presolve_level\n",
    "    if component_config.get(\"lazy_constraints_enabled\", True):\n",
    "        model.Params.LazyConstraints = 1\n",
    "    else:\n",
    "        model.Params.LazyConstraints = 0\n",
    "    model.Params.MIPFocus  = 1\n",
    "    model.Params.Cuts      = 2\n",
    "    model.Params.Symmetry  = 2\n",
    "\n",
    "def lazy_transitivity_callback(model: gp.Model, where):\n",
    "    if where != GRB.Callback.MIPSOL:\n",
    "        return\n",
    "    a = model._a\n",
    "    nodes = model._nodes\n",
    "    aval = {}\n",
    "    try:\n",
    "        for i in nodes:\n",
    "            for j in nodes:\n",
    "                if i != j:\n",
    "                    aval[(i, j)] = model.cbGetSolution(a[i, j])\n",
    "    except:\n",
    "        return\n",
    "    violations_added = 0\n",
    "    for i in nodes:\n",
    "        for j in nodes:\n",
    "            if i == j or aval.get((i, j), 0) < 0.5:\n",
    "                continue\n",
    "            for k in nodes:\n",
    "                if k != i and k != j:\n",
    "                    if (aval.get((j, k), 0) > 0.5 and aval.get((i, k), 0) < 0.5):\n",
    "                        model.cbLazy(a[i, k] >= a[i, j] + a[j, k] - 1)\n",
    "                        violations_added += 1\n",
    "                        if violations_added >= 10:\n",
    "                            return\n",
    "\n",
    "print(\"Testing MIPStart generation...\")\n",
    "\n",
    "if test_instances:\n",
    "    test_name, test_dataset, test_G, test_stats = test_instances[0]\n",
    "    print(f\"\\nTest instance: {test_name} (n={test_stats['n']}, m={test_stats['m']})\")\n",
    "    det_solution = build_deterministic_dfs_mipstart(test_G)\n",
    "    print(f\"  Deterministic DFS height: {det_solution['H']}\")\n",
    "    rand_solution = build_randomized_dfs_mipstart(test_G, seed=42)\n",
    "    print(f\"  Randomized DFS height: {rand_solution['H']}\")\n",
    "    multi_solution = build_multiple_mipstarts(test_G, num_runs=5)\n",
    "    print(f\"  Best height from multiple randomized runs: {multi_solution['H']}\")\n",
    "    nodes = list(test_G.nodes())\n",
    "    if nodes:\n",
    "        depths = [det_solution['depth'][v] for v in nodes if det_solution['depth'][v] is not None]\n",
    "        roots = [v for v in nodes if det_solution['root'][v] == 1]\n",
    "        print(f\"  Solution check: depth range=[{min(depths) if depths else 0}, {max(depths) if depths else 0}], number of roots={len(roots)}\")\n",
    "\n",
    "print(\"\\nTesting solver configuration...\")\n",
    "test_configs = [\n",
    "    {\"name\": \"Base config\", \"mipstart_enabled\": True, \"lazy_constraints_enabled\": True, \"gurobi_presolve\": 2},\n",
    "    {\"name\": \"No MIPStart\", \"mipstart_enabled\": False, \"lazy_constraints_enabled\": True, \"gurobi_presolve\": 2},\n",
    "    {\"name\": \"No Lazy constraints\", \"mipstart_enabled\": True, \"lazy_constraints_enabled\": False, \"gurobi_presolve\": 2},\n",
    "    {\"name\": \"No Presolve\", \"mipstart_enabled\": True, \"lazy_constraints_enabled\": True, \"gurobi_presolve\": 0},\n",
    "]\n",
    "\n",
    "for config in test_configs:\n",
    "    print(f\"  {config['name']}: MIP={config['mipstart_enabled']}, Lazy={config['lazy_constraints_enabled']}, Presolve={config['gurobi_presolve']}\")\n",
    "\n",
    "print(\"\\nMIPStart and solver configuration tests complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10d8cc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ILP solver...\n",
      "\n",
      "Test instance: Bull (n=5, m=5)\n",
      "  Result: treedepth = 3\n",
      "  Timeout: False\n",
      "  Solve time: 0.005s\n",
      "  Variables: 61\n",
      "  Constraints: 120\n",
      "  Preprocessing reduction: 0.0%\n",
      "\n",
      " ILP core solver tests complete\n"
     ]
    }
   ],
   "source": [
    "def solve_single_component_ilp(G: nx.Graph, \n",
    "                              component_config: Dict[str, Any],\n",
    "                              encoding_variant: str = \"v1_baseline\",\n",
    "                              mipstart_runs: int = 10) -> Tuple[Optional[int], bool, Dict[str, Any]]:\n",
    "    n = G.number_of_nodes()\n",
    "    if n <= 1:\n",
    "        return n, False, {\"variables\": 0, \"constraints\": 0, \"nodes\": 0, \"gap\": 0.0}\n",
    "\n",
    "    nodes = list(G.nodes())\n",
    "    edges = list(G.edges())\n",
    "    LB, UB = estimate_bounds(G)\n",
    "    U = UB if UB > 0 else n\n",
    "\n",
    "    model = gp.Model(f\"Treedepth_ILP_{encoding_variant}\")\n",
    "    setup_gurobi_model(model, component_config)\n",
    "\n",
    "    d = model.addVars(nodes, vtype=GRB.INTEGER, lb=1, ub=U, name=\"d\")\n",
    "    r = model.addVars(nodes, vtype=GRB.BINARY, name=\"r\")\n",
    "    p = model.addVars(nodes, nodes, vtype=GRB.BINARY, name=\"p\")\n",
    "    a = model.addVars(nodes, nodes, vtype=GRB.BINARY, name=\"a\")\n",
    "\n",
    "    try:\n",
    "        model.setObjective(gp.max_([d[v] for v in nodes]), GRB.MINIMIZE)\n",
    "        use_auxiliary_var = False\n",
    "    except:\n",
    "        max_depth = model.addVar(vtype=GRB.INTEGER, lb=LB, ub=UB, name=\"max_depth\")\n",
    "        model.setObjective(max_depth, GRB.MINIMIZE)\n",
    "        for v in nodes:\n",
    "            model.addConstr(max_depth >= d[v], name=f\"max_depth_{v}\")\n",
    "        use_auxiliary_var = True\n",
    "\n",
    "    for i in nodes:\n",
    "        model.addConstr(p[i, i] == 0, name=f\"no_self_parent_{i}\")\n",
    "        model.addConstr(a[i, i] == 0, name=f\"no_self_ancestor_{i}\")\n",
    "\n",
    "    for v in nodes:\n",
    "        model.addConstr(gp.quicksum(p[u, v] for u in nodes if u != v) + r[v] == 1, \n",
    "                       name=f\"unique_parent_{v}\")\n",
    "\n",
    "    for v in nodes:\n",
    "        model.addConstr(d[v] <= 1 + U * (1 - r[v]), name=f\"root_depth_ub_{v}\")\n",
    "        model.addConstr(d[v] >= 1 - U * (1 - r[v]), name=f\"root_depth_lb_{v}\")\n",
    "\n",
    "    for u in nodes:\n",
    "        for v in nodes:\n",
    "            if u != v:\n",
    "                model.addConstr(d[v] - d[u] >= 1 - U * (1 - p[u, v]), \n",
    "                               name=f\"parent_depth_lb_{u}_{v}\")\n",
    "                model.addConstr(d[v] - d[u] <= 1 + U * (1 - p[u, v]), \n",
    "                               name=f\"parent_depth_ub_{u}_{v}\")\n",
    "\n",
    "    for u in nodes:\n",
    "        for v in nodes:\n",
    "            if u != v:\n",
    "                model.addConstr(d[u] + 1 <= d[v] + U * (1 - a[u, v]), \n",
    "                               name=f\"ancestor_depth_{u}_{v}\")\n",
    "\n",
    "    for v in nodes:\n",
    "        model.addConstr(gp.quicksum(a[u, v] for u in nodes if u != v) == d[v] - 1, \n",
    "                       name=f\"ancestor_count_{v}\")\n",
    "\n",
    "    for u in nodes:\n",
    "        for v in nodes:\n",
    "            if u != v:\n",
    "                model.addConstr(a[u, v] >= p[u, v], name=f\"ancestor_includes_parent_{u}_{v}\")\n",
    "\n",
    "    if encoding_variant in [\"v2_explicit_transitivity\", \"v4_with_symmetry\", \"v5_enhanced_constraints\"]:\n",
    "        transitivity_count = 0\n",
    "        for i in nodes:\n",
    "            for j in nodes:\n",
    "                if i != j:\n",
    "                    for k in nodes:\n",
    "                        if k != i and k != j:\n",
    "                            model.addConstr(a[i, k] >= a[i, j] + a[j, k] - 1, \n",
    "                                           name=f\"transitivity_{i}_{j}_{k}\")\n",
    "                            transitivity_count += 1\n",
    "                if transitivity_count >= n * n:\n",
    "                    break\n",
    "            if transitivity_count >= n * n:\n",
    "                break\n",
    "\n",
    "    if encoding_variant in [\"v3_tight_bounds\", \"v5_enhanced_constraints\"]:\n",
    "        for v in nodes:\n",
    "            neighbors = list(G.neighbors(v))\n",
    "            if neighbors:\n",
    "                model.addConstr(gp.quicksum(a[u, v] + a[v, u] for u in neighbors) >= 1,\n",
    "                               name=f\"tight_neighbor_{v}\")\n",
    "\n",
    "    if encoding_variant in [\"v4_with_symmetry\", \"v5_enhanced_constraints\"]:\n",
    "        for i in nodes:\n",
    "            for j in nodes:\n",
    "                if i < j:\n",
    "                    model.addConstr(a[i, j] + a[j, i] <= 1, name=f\"symmetry_break_{i}_{j}\")\n",
    "\n",
    "    for u, v in edges:\n",
    "        model.addConstr(a[u, v] + a[v, u] >= 1, name=f\"edge_constraint_{u}_{v}\")\n",
    "\n",
    "    solve_stats = {\"variables\": 0, \"constraints\": 0, \"nodes\": 0, \"gap\": 0.0}\n",
    "    \n",
    "    if component_config.get(\"mipstart_enabled\", True):\n",
    "        try:\n",
    "            ms = build_multiple_mipstarts(G, EXPERIMENTAL_CONFIG['MIPSTART_RUNS'])\n",
    "            if not use_auxiliary_var:\n",
    "                pass\n",
    "            else:\n",
    "                max_depth.Start = ms[\"H\"]\n",
    "            for v in nodes:\n",
    "                d[v].Start = ms[\"depth\"][v]\n",
    "                r[v].Start = ms[\"root\"][v]\n",
    "                for u in nodes:\n",
    "                    if u != v:\n",
    "                        p[u, v].Start = 1 if ms[\"parent\"][v] == u else 0\n",
    "                        a[u, v].Start = ms[\"ancestor\"][(u, v)]\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: Failed to set MIPStart: {e}\")\n",
    "\n",
    "    if component_config.get(\"lazy_constraints_enabled\", True):\n",
    "        model._a = a\n",
    "        model._nodes = nodes\n",
    "        callback_func = lambda m, w: lazy_transitivity_callback(m, w)\n",
    "    else:\n",
    "        callback_func = None\n",
    "        print(f\"    Adding explicit transitivity constraints...\", end=\"\")\n",
    "        transitivity_constraints_added = 0\n",
    "        for i in nodes:\n",
    "            for j in nodes:\n",
    "                if i != j:\n",
    "                    for k in nodes:\n",
    "                        if k != i and k != j:\n",
    "                            model.addConstr(a[i, k] >= a[i, j] + a[j, k] - 1, \n",
    "                                           name=f\"explicit_transitivity_{i}_{j}_{k}\")\n",
    "                            transitivity_constraints_added += 1\n",
    "        print(f\" {transitivity_constraints_added}\")\n",
    "\n",
    "    try:\n",
    "        if callback_func:\n",
    "            model.optimize(callback_func)\n",
    "        else:\n",
    "            model.optimize()\n",
    "        solve_stats = {\n",
    "            \"variables\": model.NumVars,\n",
    "            \"constraints\": model.NumConstrs,\n",
    "            \"nodes\": int(model.NodeCount) if hasattr(model, 'NodeCount') else 0,\n",
    "            \"gap\": model.MIPGap if hasattr(model, 'MIPGap') else 0.0,\n",
    "            \"obj_val\": model.ObjVal if model.Status == GRB.OPTIMAL else None,\n",
    "            \"sol_count\": model.SolCount if hasattr(model, 'SolCount') else 0\n",
    "        }\n",
    "        if model.Status == GRB.OPTIMAL:\n",
    "            result = int(max(d[v].X for v in nodes))\n",
    "            return result, False, solve_stats\n",
    "        elif model.Status == GRB.TIME_LIMIT and model.SolCount > 0:\n",
    "            result = int(max(d[v].X for v in nodes))\n",
    "            return result, True, solve_stats\n",
    "        else:\n",
    "            return None, True, solve_stats\n",
    "    except Exception as e:\n",
    "        print(f\"    Solve exception: {e}\")\n",
    "        return None, True, solve_stats\n",
    "\n",
    "def solve_treedepth_ilp(G: nx.Graph,\n",
    "                       component_config: Dict[str, Any],\n",
    "                       encoding_variant: str = \"v1_baseline\",\n",
    "                       enable_preprocessing: bool = True,\n",
    "                       mipstart_runs: int = 10) -> Tuple[Optional[int], bool, Dict[str, Any]]:\n",
    "    start_time = time.time()\n",
    "    processed_G, apex_buffer, preprocess_stats = preprocess_graph(G, enable_preprocessing)\n",
    "    if processed_G.number_of_nodes() <= 1:\n",
    "        solve_time = time.time() - start_time\n",
    "        stats = {\n",
    "            **preprocess_stats,\n",
    "            \"solve_time\": solve_time,\n",
    "            \"components_solved\": 1,\n",
    "            \"total_variables\": 0,\n",
    "            \"total_constraints\": 0,\n",
    "            \"max_nodes\": 0,\n",
    "            \"avg_gap\": 0.0\n",
    "        }\n",
    "        return processed_G.number_of_nodes() + apex_buffer, False, stats\n",
    "\n",
    "    max_td = 0\n",
    "    overall_timeout = False\n",
    "    total_vars = 0\n",
    "    total_constrs = 0\n",
    "    total_nodes = 0\n",
    "    gaps = []\n",
    "    components_count = 0\n",
    "    \n",
    "    for comp in nx.connected_components(processed_G):\n",
    "        sub_G = processed_G.subgraph(comp).copy()\n",
    "        sub_G = nx.convert_node_labels_to_integers(sub_G, first_label=0, ordering=\"default\")\n",
    "        td, timeout, comp_stats = solve_single_component_ilp(\n",
    "            sub_G, component_config, encoding_variant, mipstart_runs\n",
    "        )\n",
    "        if td is None:\n",
    "            solve_time = time.time() - start_time\n",
    "            stats = {\n",
    "                **preprocess_stats,\n",
    "                \"solve_time\": solve_time,\n",
    "                \"components_solved\": components_count,\n",
    "                \"total_variables\": total_vars,\n",
    "                \"total_constraints\": total_constrs,\n",
    "                \"max_nodes\": total_nodes,\n",
    "                \"avg_gap\": np.mean(gaps) if gaps else 0.0\n",
    "            }\n",
    "            return None, True, stats\n",
    "        max_td = max(max_td, td)\n",
    "        if timeout:\n",
    "            overall_timeout = True\n",
    "        total_vars += comp_stats[\"variables\"]\n",
    "        total_constrs += comp_stats[\"constraints\"]\n",
    "        total_nodes += comp_stats[\"nodes\"]\n",
    "        if comp_stats[\"gap\"] > 0:\n",
    "            gaps.append(comp_stats[\"gap\"])\n",
    "        components_count += 1\n",
    "    \n",
    "    final_td = max_td + apex_buffer\n",
    "    solve_time = time.time() - start_time\n",
    "    final_stats = {\n",
    "        **preprocess_stats,\n",
    "        \"solve_time\": solve_time,\n",
    "        \"components_solved\": components_count,\n",
    "        \"total_variables\": total_vars,\n",
    "        \"total_constraints\": total_constrs,\n",
    "        \"max_nodes\": total_nodes,\n",
    "        \"avg_gap\": np.mean(gaps) if gaps else 0.0\n",
    "    }\n",
    "    return final_td, overall_timeout, final_stats\n",
    "\n",
    "print(\"Testing ILP solver...\")\n",
    "\n",
    "test_config = {\n",
    "    \"mipstart_enabled\": True,\n",
    "    \"lazy_constraints_enabled\": True,\n",
    "    \"gurobi_presolve\": 2\n",
    "}\n",
    "\n",
    "small_instances = [(name, dataset, G, stats) for name, dataset, G, stats in test_instances if stats[\"n\"] <= 8]\n",
    "\n",
    "if small_instances:\n",
    "    test_name, test_dataset, test_G, test_stats = small_instances[0]\n",
    "    print(f\"\\nTest instance: {test_name} (n={test_stats['n']}, m={test_stats['m']})\")\n",
    "    result, timeout, stats = solve_treedepth_ilp(\n",
    "        test_G, test_config, \"v1_baseline\", enable_preprocessing=True, mipstart_runs=3\n",
    "    )\n",
    "    if result is not None:\n",
    "        print(f\"  Result: treedepth = {result}\")\n",
    "        print(f\"  Timeout: {timeout}\")\n",
    "        print(f\"  Solve time: {stats['solve_time']:.3f}s\")\n",
    "        print(f\"  Variables: {stats['total_variables']}\")\n",
    "        print(f\"  Constraints: {stats['total_constraints']}\")\n",
    "        print(f\"  Preprocessing reduction: {stats['reduction_rate']:.1%}\")\n",
    "    else:\n",
    "        print(\"  Solve failed\")\n",
    "\n",
    "print(\"\\n ILP core solver tests complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c581b934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Experiment 4.6: Optimal ILP vs SAT performance comparison (unified config)\n",
      "============================================================\n",
      "Parameters:\n",
      "  max_nodes: 50\n",
      "  max_edges: 10000\n",
      "  min_nodes: 3\n",
      "  timeout_per_instance: 1200\n",
      "  buffer_time: 10\n",
      "  skip_large_instances: True\n",
      "  enable_progress_display: True\n",
      "  save_intermediate_results: False\n",
      "  max_density: 1.0\n",
      "  min_density: 0.0\n",
      "\n",
      " Notes on parameter tweaks:\n",
      "  To modify parameters, edit the EXPERIMENT_4_6_CONFIG dictionary above\n",
      "  Key parameters:\n",
      "    timeout_per_instance: time limit per instance (seconds)\n",
      "    max_nodes: maximum number of nodes\n",
      "    max_edges: maximum number of edges\n",
      "    skip_large_instances: whether to skip large instances\n",
      "\n",
      "Running Experiment 4.6 with unified configuration (accuracy first)...\n",
      "Using hardcoded best result from Experiment 4.5:\n",
      "  Best encoding variant: v5_enhanced_constraints\n",
      "  Accuracy: 77.1%\n",
      "  Config name: presolve_only\n",
      "  Preprocessing: False\n",
      "  MIPStart: False\n",
      "  Lazy constraints: False\n",
      "  Gurobi presolve: 2\n",
      "\n",
      "Running with unified configuration:\n",
      "  Time limit: 1200s (20.0min)\n",
      "  Node limit: 3-50\n",
      "  Edge limit: ≤10000\n",
      "  Density range: [0.00, 1.00]\n",
      "  Golden-standard instances: 35\n",
      "\n",
      "Optimal ILP configuration (from Experiment 4.5 hardcoded result):\n",
      "  Encoding variant: v5_enhanced_constraints\n",
      "  Accuracy: 77.1%\n",
      "  Preprocessing: False\n",
      "\n",
      "Checking SAT solver environment...\n",
      "Glucose SAT solver available\n",
      "Instance filtering result:\n",
      "  Original instances: 33\n",
      "  Selected instances: 33\n",
      "  Skip stats:\n",
      "\n",
      "Size distribution of selected instances:\n",
      "  TINY (≤10 nodes): 9 instances\n",
      "  SMALL (11-20 nodes): 22 instances\n",
      "  MEDIUM (21-30 nodes): 2 instances\n",
      "\n",
      "Time estimate:\n",
      "  Maximum possible runtime: 22.0 hours\n",
      "  Expected actual runtime: 8.8 hours (rough 40% estimate)\n",
      "\n",
      "Starting ILP vs SAT comparison test (33 selected instances)...\n",
      "\n",
      "[1/33] Testing instance: famous/Brinkmann (MEDIUM)\n",
      "  Graph size: n=21, m=42, density=0.200\n",
      "  Elapsed: 0.0 min\n",
      "    [ILP]     Adding explicit transitivity constraints... 7980\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import tempfile\n",
    "import shutil\n",
    "import threading\n",
    "import signal\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "EXPERIMENT_4_6_CONFIG = {\n",
    "    \"max_nodes\": 50,\n",
    "    \"max_edges\": 10000,\n",
    "    \"min_nodes\": 3,\n",
    "    \"timeout_per_instance\": 1200,\n",
    "    \"buffer_time\": 10,\n",
    "    \"skip_large_instances\": True,\n",
    "    \"enable_progress_display\": True,\n",
    "    \"save_intermediate_results\": False,\n",
    "    \"max_density\": 1.0,\n",
    "    \"min_density\": 0.0,\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Experiment 4.6: Optimal ILP vs SAT performance comparison (unified config)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Parameters:\")\n",
    "for key, value in EXPERIMENT_4_6_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print()\n",
    "\n",
    "def filter_instances_by_config(test_instances, config):\n",
    "    selected_instances = []\n",
    "    skipped_stats = {\n",
    "        \"too_many_nodes\": 0,\n",
    "        \"too_many_edges\": 0, \n",
    "        \"too_few_nodes\": 0,\n",
    "        \"density_out_of_range\": 0,\n",
    "        \"large_instance_skip\": 0\n",
    "    }\n",
    "    for name, dataset, G, stats in test_instances:\n",
    "        if stats[\"n\"] > config[\"max_nodes\"]:\n",
    "            skipped_stats[\"too_many_nodes\"] += 1\n",
    "            continue\n",
    "        if stats[\"n\"] < config[\"min_nodes\"]:\n",
    "            skipped_stats[\"too_few_nodes\"] += 1\n",
    "            continue\n",
    "        if stats[\"m\"] > config[\"max_edges\"]:\n",
    "            skipped_stats[\"too_many_edges\"] += 1\n",
    "            continue\n",
    "        density = stats[\"density\"]\n",
    "        if density > config[\"max_density\"] or density < config[\"min_density\"]:\n",
    "            skipped_stats[\"density_out_of_range\"] += 1\n",
    "            continue\n",
    "        if config[\"skip_large_instances\"] and stats[\"n\"] > 30:\n",
    "            skipped_stats[\"large_instance_skip\"] += 1\n",
    "            continue\n",
    "        selected_instances.append((name, dataset, G, stats))\n",
    "    return selected_instances, skipped_stats\n",
    "\n",
    "def solve_with_strict_timeout(solve_func, timeout, *args, **kwargs):\n",
    "    result = {\"td\": None, \"timeout\": True, \"stats\": {}, \"exception\": None}\n",
    "    def target():\n",
    "        try:\n",
    "            result[\"td\"], result[\"timeout\"], result[\"stats\"] = solve_func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            result[\"exception\"] = e\n",
    "    thread = threading.Thread(target=target)\n",
    "    thread.daemon = True\n",
    "    thread.start()\n",
    "    thread.join(timeout)\n",
    "    if thread.is_alive():\n",
    "        result[\"timeout\"] = True\n",
    "    return result[\"td\"], result[\"timeout\"], result[\"stats\"]\n",
    "\n",
    "def prepare_sat_solver():\n",
    "    print(\"Checking SAT solver environment...\")\n",
    "    try:\n",
    "        result = subprocess.run(['glucose', '-h'], capture_output=True, text=True, timeout=5)\n",
    "        print(\"Glucose SAT solver available\")\n",
    "        return True\n",
    "    except (FileNotFoundError, subprocess.TimeoutExpired):\n",
    "        print(\"Glucose solver not found, skipping SAT comparison\")\n",
    "        return False\n",
    "\n",
    "def make_vars(g, width):\n",
    "    nv = g.number_of_nodes()\n",
    "    p = [[[0]*width for _ in range(nv)] for __ in range(nv)]\n",
    "    cur = 1\n",
    "    for u in range(nv):\n",
    "        for v in range(u, nv):\n",
    "            for i in range(width):\n",
    "                p[u][v][i] = cur\n",
    "                cur += 1\n",
    "    return p, cur-1\n",
    "\n",
    "def generate_sat_encoding(g, width):\n",
    "    s, nvar = make_vars(g, width)\n",
    "    clauses = []\n",
    "    nv = g.number_of_nodes()\n",
    "    nclauses = 0\n",
    "    for u in range(nv):\n",
    "        for v in range(u, nv):\n",
    "            clauses.append(f\"{s[u][v][width-1]} 0\")\n",
    "            nclauses += 1\n",
    "            clauses.append(f\"-{s[u][v][0]} 0\")\n",
    "            nclauses += 1\n",
    "    for u in range(nv):\n",
    "        for v in range(u, nv):\n",
    "            for i in range(1, width):\n",
    "                clauses.append(f\"-{s[u][v][i-1]} {s[u][v][i]} 0\")\n",
    "                nclauses += 1\n",
    "    for u in range(nv):\n",
    "        for v in range(u+1, nv):\n",
    "            for w in range(v+1, nv):\n",
    "                for i in range(width):\n",
    "                    clauses.append(f\"-{s[u][v][i]} -{s[u][w][i]} {s[v][w][i]} 0\")\n",
    "                    clauses.append(f\"-{s[u][v][i]} -{s[v][w][i]} {s[u][w][i]} 0\")\n",
    "                    clauses.append(f\"-{s[u][w][i]} -{s[v][w][i]} {s[u][v][i]} 0\")\n",
    "                    nclauses += 3\n",
    "    for u in range(nv):\n",
    "        for v in range(u+1, nv):\n",
    "            for i in range(width):\n",
    "                clauses.append(f\"-{s[u][v][i]} {s[u][u][i]} 0\")\n",
    "                clauses.append(f\"-{s[u][v][i]} {s[v][v][i]} 0\")\n",
    "                nclauses += 2\n",
    "    for u in range(nv):\n",
    "        for v in range(u+1, nv):\n",
    "            for i in range(1, width):\n",
    "                clauses.append(f\"-{s[u][v][i]} {s[u][u][i-1]} {s[v][v][i-1]} 0\")\n",
    "                nclauses += 1\n",
    "    for (u, v) in g.edges():\n",
    "        if u > v:\n",
    "            u, v = v, u\n",
    "        for i in range(1, width):\n",
    "            clauses.append(f\"-{s[u][u][i]} {s[u][u][i-1]} -{s[v][v][i]} {s[u][v][i]} 0\")\n",
    "            clauses.append(f\"-{s[u][u][i]} {s[v][v][i-1]} -{s[v][v][i]} {s[u][v][i]} 0\")\n",
    "            nclauses += 2\n",
    "    preamble = f\"p cnf {nvar} {nclauses}\"\n",
    "    return preamble + \"\\n\" + \"\\n\".join(clauses) + \"\\n\"\n",
    "\n",
    "def solve_component_sat_with_timeout(g, timeout=120, temp_dir=None):\n",
    "    if temp_dir is None:\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "    start_time = time.time()\n",
    "    encoding_times = []\n",
    "    solving_times = []\n",
    "    n = g.number_of_nodes()\n",
    "    if n <= 1:\n",
    "        return n, False, encoding_times, solving_times, 0, 0\n",
    "    timeout_occurred = False\n",
    "    variables_generated = 0\n",
    "    clauses_generated = 0\n",
    "    for w in range(n+1, 1, -1):\n",
    "        elapsed_time = time.time() - start_time\n",
    "        if elapsed_time >= timeout:\n",
    "            timeout_occurred = True\n",
    "            break\n",
    "        encoding_start = time.time()\n",
    "        cnf_str = generate_sat_encoding(g, w)\n",
    "        encoding_time = time.time() - encoding_start\n",
    "        encoding_times.append(encoding_time)\n",
    "        lines = cnf_str.strip().split('\\n')\n",
    "        header = lines[0]\n",
    "        parts = header.split()\n",
    "        if len(parts) >= 4:\n",
    "            variables_generated = int(parts[2])\n",
    "            clauses_generated = int(parts[3])\n",
    "        cnf_file = os.path.join(temp_dir, f'temp_{w}.cnf')\n",
    "        with open(cnf_file, 'w') as f:\n",
    "            f.write(cnf_str)\n",
    "        remaining_time = timeout - elapsed_time\n",
    "        if remaining_time <= 0:\n",
    "            timeout_occurred = True\n",
    "            break\n",
    "        sol_file = os.path.join(temp_dir, f'temp_{w}.sol')\n",
    "        cmd = ['glucose', f'-cpu-lim={int(remaining_time)}', cnf_file, sol_file]\n",
    "        solving_start = time.time()\n",
    "        try:\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True, \n",
    "                                  timeout=remaining_time+5)\n",
    "            solving_time = time.time() - solving_start\n",
    "            solving_times.append(solving_time)\n",
    "            rc = result.returncode\n",
    "            if rc == 0:\n",
    "                timeout_occurred = True\n",
    "            elif rc == 20:\n",
    "                try:\n",
    "                    os.remove(cnf_file)\n",
    "                    if os.path.exists(sol_file):\n",
    "                        os.remove(sol_file)\n",
    "                except:\n",
    "                    pass\n",
    "                return w, timeout_occurred, encoding_times, solving_times, variables_generated, clauses_generated\n",
    "        except subprocess.TimeoutExpired:\n",
    "            solving_time = time.time() - solving_start\n",
    "            solving_times.append(solving_time)\n",
    "            timeout_occurred = True\n",
    "        try:\n",
    "            os.remove(cnf_file)\n",
    "            if os.path.exists(sol_file):\n",
    "                os.remove(sol_file)\n",
    "        except:\n",
    "            pass\n",
    "    return 1, timeout_occurred, encoding_times, solving_times, variables_generated, clauses_generated\n",
    "\n",
    "def solve_treedepth_sat_with_strict_timeout(G, timeout=120, enable_preprocessing=True):\n",
    "    start_time = time.time()\n",
    "    processed_G, apex_buffer, preprocess_stats = preprocess_graph(G, enable_preprocessing)\n",
    "    if processed_G.number_of_nodes() <= 1:\n",
    "        solve_time = time.time() - start_time\n",
    "        stats = {\n",
    "            **preprocess_stats,\n",
    "            \"solve_time\": solve_time,\n",
    "            \"components_solved\": 1,\n",
    "            \"total_encoding_time\": 0,\n",
    "            \"total_solving_time\": 0,\n",
    "            \"sat_variables\": 0,\n",
    "            \"sat_clauses\": 0\n",
    "        }\n",
    "        return processed_G.number_of_nodes() + apex_buffer, False, stats\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    try:\n",
    "        max_td = 0\n",
    "        overall_timeout = False\n",
    "        total_encoding_time = 0\n",
    "        total_solving_time = 0\n",
    "        max_variables = 0\n",
    "        max_clauses = 0\n",
    "        components_count = 0\n",
    "        for comp in nx.connected_components(processed_G):\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if elapsed_time >= timeout:\n",
    "                overall_timeout = True\n",
    "                break\n",
    "            sub_G = processed_G.subgraph(comp).copy()\n",
    "            sub_G = nx.convert_node_labels_to_integers(sub_G, first_label=0, ordering=\"default\")\n",
    "            remaining_timeout = timeout - elapsed_time\n",
    "            td, comp_timeout, enc_times, sol_times, variables, clauses = solve_component_sat_with_timeout(\n",
    "                sub_G, remaining_timeout, temp_dir\n",
    "            )\n",
    "            if td is None:\n",
    "                solve_time = time.time() - start_time\n",
    "                stats = {\n",
    "                    **preprocess_stats,\n",
    "                    \"solve_time\": solve_time,\n",
    "                    \"components_solved\": components_count,\n",
    "                    \"total_encoding_time\": total_encoding_time,\n",
    "                    \"total_solving_time\": total_solving_time,\n",
    "                    \"sat_variables\": max_variables,\n",
    "                    \"sat_clauses\": max_clauses\n",
    "                }\n",
    "                return None, True, stats\n",
    "            max_td = max(max_td, td)\n",
    "            if comp_timeout:\n",
    "                overall_timeout = True\n",
    "            total_encoding_time += sum(enc_times)\n",
    "            total_solving_time += sum(sol_times)\n",
    "            max_variables = max(max_variables, variables)\n",
    "            max_clauses = max(max_clauses, clauses)\n",
    "            components_count += 1\n",
    "        final_solve_time = time.time() - start_time\n",
    "        if final_solve_time >= timeout:\n",
    "            overall_timeout = True\n",
    "        final_td = max_td + apex_buffer\n",
    "        final_stats = {\n",
    "            **preprocess_stats,\n",
    "            \"solve_time\": final_solve_time,\n",
    "            \"components_solved\": components_count,\n",
    "            \"total_encoding_time\": total_encoding_time,\n",
    "            \"total_solving_time\": total_solving_time,\n",
    "            \"sat_variables\": max_variables,\n",
    "            \"sat_clauses\": max_clauses\n",
    "        }\n",
    "        return final_td, overall_timeout, final_stats\n",
    "    finally:\n",
    "        try:\n",
    "            shutil.rmtree(temp_dir)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "def run_ilp_vs_sat_experiment_with_unified_config():\n",
    "    global OPTIMAL_ILP_CONFIG\n",
    "    OPTIMAL_ILP_CONFIG = {\n",
    "        'config_name': 'presolve_only',\n",
    "        'config_description': 'Only Gurobi presolve - from Experiment 4.4',\n",
    "        'accuracy_rate': 0.771,\n",
    "        'encoding_variant': 'v5_enhanced_constraints',\n",
    "        'preprocessing_enabled': False,\n",
    "        'mipstart_enabled': False,\n",
    "        'lazy_constraints_enabled': False,\n",
    "        'gurobi_presolve': 2\n",
    "    }\n",
    "    print(\"Using hardcoded best result from Experiment 4.5:\")\n",
    "    print(f\"  Best encoding variant: {OPTIMAL_ILP_CONFIG['encoding_variant']}\")\n",
    "    print(f\"  Accuracy: {OPTIMAL_ILP_CONFIG['accuracy_rate']:.1%}\")\n",
    "    print(f\"  Config name: {OPTIMAL_ILP_CONFIG['config_name']}\")\n",
    "    print(f\"  Preprocessing: {OPTIMAL_ILP_CONFIG['preprocessing_enabled']}\")\n",
    "    print(f\"  MIPStart: {OPTIMAL_ILP_CONFIG['mipstart_enabled']}\")\n",
    "    print(f\"  Lazy constraints: {OPTIMAL_ILP_CONFIG['lazy_constraints_enabled']}\")\n",
    "    print(f\"  Gurobi presolve: {OPTIMAL_ILP_CONFIG['gurobi_presolve']}\")\n",
    "    print()\n",
    "    config = EXPERIMENT_4_6_CONFIG\n",
    "    print(\"Running with unified configuration:\")\n",
    "    print(f\"  Time limit: {config['timeout_per_instance']}s ({config['timeout_per_instance']/60:.1f}min)\")\n",
    "    print(f\"  Node limit: {config['min_nodes']}-{config['max_nodes']}\")\n",
    "    print(f\"  Edge limit: ≤{config['max_edges']}\")\n",
    "    print(f\"  Density range: [{config['min_density']:.2f}, {config['max_density']:.2f}]\")\n",
    "    print(f\"  Golden-standard instances: {len(GOLDEN_STANDARD) if 'GOLDEN_STANDARD' in globals() else 'undefined'}\")\n",
    "    print()\n",
    "    optimal_ilp_config = OPTIMAL_ILP_CONFIG.copy()\n",
    "    print(\"Optimal ILP configuration (from Experiment 4.5 hardcoded result):\")\n",
    "    print(f\"  Encoding variant: {optimal_ilp_config['encoding_variant']}\")\n",
    "    print(f\"  Accuracy: {optimal_ilp_config['accuracy_rate']:.1%}\")\n",
    "    print(f\"  Preprocessing: {optimal_ilp_config['preprocessing_enabled']}\")\n",
    "    print()\n",
    "    sat_available = prepare_sat_solver()\n",
    "    if not sat_available:\n",
    "        print(\"SAT solver unavailable; running ILP only...\")\n",
    "        return None, None\n",
    "    try:\n",
    "        selected_instances, skipped_stats = filter_instances_by_config(test_instances, config)\n",
    "    except NameError:\n",
    "        print(\"Error: test_instances is not defined; please ensure test instances are loaded\")\n",
    "        return None, None\n",
    "    print(\"Instance filtering result:\")\n",
    "    print(f\"  Original instances: {len(test_instances)}\")\n",
    "    print(f\"  Selected instances: {len(selected_instances)}\")\n",
    "    print(\"  Skip stats:\")\n",
    "    for reason, count in skipped_stats.items():\n",
    "        if count > 0:\n",
    "            reason_desc = {\n",
    "                \"too_many_nodes\": f\"nodes > {config['max_nodes']}\",\n",
    "                \"too_many_edges\": f\"edges > {config['max_edges']}\",\n",
    "                \"too_few_nodes\": f\"nodes < {config['min_nodes']}\",\n",
    "                \"density_out_of_range\": \"density out of range\",\n",
    "                \"large_instance_skip\": \"large instance skipped\"\n",
    "            }\n",
    "            print(f\"    {reason_desc[reason]}: {count}\")\n",
    "    def categorize_instance_size(n):\n",
    "        if n <= 10:\n",
    "            return \"TINY\"\n",
    "        elif n <= 20:\n",
    "            return \"SMALL\" \n",
    "        elif n <= 30:\n",
    "            return \"MEDIUM\"\n",
    "        else:\n",
    "            return \"LARGE\"\n",
    "    size_categories = {\"TINY\": [], \"SMALL\": [], \"MEDIUM\": [], \"LARGE\": []}\n",
    "    for name, dataset, G, stats in selected_instances:\n",
    "        category = categorize_instance_size(stats[\"n\"])\n",
    "        size_categories[category].append((name, dataset, G, stats))\n",
    "    print(\"\\nSize distribution of selected instances:\")\n",
    "    for category, instances in size_categories.items():\n",
    "        if instances:\n",
    "            size_range = {\"TINY\": \"≤10\", \"SMALL\": \"11-20\", \"MEDIUM\": \"21-30\", \"LARGE\": \"31-50\"}[category]\n",
    "            print(f\"  {category} ({size_range} nodes): {len(instances)} instances\")\n",
    "    total_selected = len(selected_instances)\n",
    "    estimated_max_time = total_selected * 2 * config['timeout_per_instance']\n",
    "    print(\"\\nTime estimate:\")\n",
    "    print(f\"  Maximum possible runtime: {estimated_max_time/3600:.1f} hours\")\n",
    "    print(f\"  Expected actual runtime: {estimated_max_time*0.4/3600:.1f} hours (rough 40% estimate)\")\n",
    "    print()\n",
    "    results = []\n",
    "    method_performance = {\n",
    "        \"ILP\": {\"optimal\": 0, \"timeout\": 0, \"failed\": 0, \"times\": [], \"success_times\": []},\n",
    "        \"SAT\": {\"optimal\": 0, \"timeout\": 0, \"failed\": 0, \"times\": [], \"success_times\": []}\n",
    "    }\n",
    "    method_validations = {\"ILP\": [], \"SAT\": []}\n",
    "    ilp_solver_config = {\n",
    "        \"mipstart_enabled\": optimal_ilp_config['mipstart_enabled'],\n",
    "        \"lazy_constraints_enabled\": optimal_ilp_config['lazy_constraints_enabled'],\n",
    "        \"gurobi_presolve\": optimal_ilp_config['gurobi_presolve']\n",
    "    }\n",
    "    print(f\"Starting ILP vs SAT comparison test ({total_selected} selected instances)...\")\n",
    "    experiment_start_time = time.time()\n",
    "    for inst_idx, (instance, dataset, G, graph_stats) in enumerate(selected_instances, 1):\n",
    "        size_category = categorize_instance_size(graph_stats[\"n\"])\n",
    "        elapsed_time = time.time() - experiment_start_time\n",
    "        if inst_idx > 1:\n",
    "            avg_time_per_instance = elapsed_time / (inst_idx - 1)\n",
    "            remaining_instances = total_selected - inst_idx + 1\n",
    "            eta_seconds = remaining_instances * avg_time_per_instance\n",
    "            eta_str = f\", ETA: {eta_seconds/3600:.1f}h\" if eta_seconds > 3600 else f\", ETA: {eta_seconds/60:.0f}min\"\n",
    "        else:\n",
    "            eta_str = \"\"\n",
    "        print(f\"\\n[{inst_idx}/{total_selected}] Testing instance: {dataset}/{instance} ({size_category}){eta_str}\")\n",
    "        print(f\"  Graph size: n={graph_stats['n']}, m={graph_stats['m']}, density={graph_stats['density']:.3f}\")\n",
    "        print(f\"  Elapsed: {elapsed_time/60:.1f} min\")\n",
    "        print(\"    [ILP]\", end=\" \")\n",
    "        ilp_start_time = time.time()\n",
    "        try:\n",
    "            original_timeout = EXPERIMENTAL_CONFIG['TIMEOUT']\n",
    "            EXPERIMENTAL_CONFIG['TIMEOUT'] = config['timeout_per_instance'] - config['buffer_time']\n",
    "            td_ilp, timeout_ilp_internal, stats_ilp = solve_treedepth_ilp(\n",
    "                G, ilp_solver_config, optimal_ilp_config['encoding_variant'],\n",
    "                enable_preprocessing=optimal_ilp_config['preprocessing_enabled'],\n",
    "                mipstart_runs=EXPERIMENTAL_CONFIG['MIPSTART_RUNS']\n",
    "            )\n",
    "            EXPERIMENTAL_CONFIG['TIMEOUT'] = original_timeout\n",
    "            time_ilp = time.time() - ilp_start_time\n",
    "            if time_ilp >= config['timeout_per_instance']:\n",
    "                timeout_ilp = True\n",
    "                if td_ilp is not None:\n",
    "                    status_ilp = \"timeout\"\n",
    "                else:\n",
    "                    status_ilp = \"failed\"\n",
    "            else:\n",
    "                timeout_ilp = timeout_ilp_internal\n",
    "                if td_ilp is not None:\n",
    "                    if timeout_ilp:\n",
    "                        status_ilp = \"timeout\"\n",
    "                    else:\n",
    "                        status_ilp = \"optimal\"\n",
    "                else:\n",
    "                    status_ilp = \"failed\"\n",
    "            if status_ilp == \"optimal\":\n",
    "                method_performance[\"ILP\"][\"optimal\"] += 1\n",
    "                method_performance[\"ILP\"][\"success_times\"].append(time_ilp)\n",
    "                print(f\"treedepth = {td_ilp}, {time_ilp:.2f}s (optimal)\", end=\"\")\n",
    "            elif status_ilp == \"timeout\":\n",
    "                method_performance[\"ILP\"][\"timeout\"] += 1\n",
    "                print(f\"treedepth ≤ {td_ilp if td_ilp else '?'}, {time_ilp:.2f}s (timeout)\", end=\"\")\n",
    "            else:\n",
    "                method_performance[\"ILP\"][\"failed\"] += 1\n",
    "                print(f\"failed, {time_ilp:.2f}s\", end=\"\")\n",
    "            method_performance[\"ILP\"][\"times\"].append(time_ilp)\n",
    "        except Exception as e:\n",
    "            time_ilp = time.time() - ilp_start_time\n",
    "            td_ilp, timeout_ilp, status_ilp = None, True, \"failed\"\n",
    "            stats_ilp = {\"total_variables\": 0, \"total_constraints\": 0}\n",
    "            method_performance[\"ILP\"][\"failed\"] += 1\n",
    "            method_performance[\"ILP\"][\"times\"].append(time_ilp)\n",
    "            print(f\"Exception: {str(e)[:30]}\", end=\"\")\n",
    "            EXPERIMENTAL_CONFIG['TIMEOUT'] = original_timeout\n",
    "        validation_ilp = validate_result(instance, td_ilp, timeout_ilp)\n",
    "        method_validations[\"ILP\"].append(validation_ilp)\n",
    "        if validation_ilp[\"validation_status\"] != \"no_golden_standard\":\n",
    "            if validation_ilp[\"is_correct\"]:\n",
    "                print(\" ✅\")\n",
    "            else:\n",
    "                if validation_ilp[\"error_type\"] == \"timeout_not_optimal\":\n",
    "                    print(\"(timeout)\")\n",
    "                else:\n",
    "                    print(f\"({validation_ilp['error_type']})\")\n",
    "        else:\n",
    "            print(\"no\")\n",
    "        print(\"    [SAT]\", end=\" \")\n",
    "        sat_start_time = time.time()\n",
    "        try:\n",
    "            td_sat, timeout_sat, stats_sat = solve_treedepth_sat_with_strict_timeout(\n",
    "                G, timeout=config['timeout_per_instance'],\n",
    "                enable_preprocessing=optimal_ilp_config['preprocessing_enabled']\n",
    "            )\n",
    "            time_sat = time.time() - sat_start_time\n",
    "            if time_sat >= config['timeout_per_instance']:\n",
    "                timeout_sat = True\n",
    "            if td_sat is not None:\n",
    "                if timeout_sat:\n",
    "                    status_sat = \"timeout\"\n",
    "                    method_performance[\"SAT\"][\"timeout\"] += 1\n",
    "                    print(f\"treedepth ≤ {td_sat}, {time_sat:.2f}s (timeout)\", end=\"\")\n",
    "                else:\n",
    "                    status_sat = \"optimal\"\n",
    "                    method_performance[\"SAT\"][\"optimal\"] += 1\n",
    "                    method_performance[\"SAT\"][\"success_times\"].append(time_sat)\n",
    "                    print(f\"treedepth = {td_sat}, {time_sat:.2f}s (optimal)\", end=\"\")\n",
    "            else:\n",
    "                status_sat = \"failed\"\n",
    "                method_performance[\"SAT\"][\"failed\"] += 1\n",
    "                print(f\"failed, {time_sat:.2f}s\", end=\"\")\n",
    "            method_performance[\"SAT\"][\"times\"].append(time_sat)\n",
    "        except Exception as e:\n",
    "            time_sat = time.time() - sat_start_time\n",
    "            td_sat, timeout_sat, status_sat = None, True, \"failed\"\n",
    "            stats_sat = {\"sat_variables\": 0, \"sat_clauses\": 0}\n",
    "            method_performance[\"SAT\"][\"failed\"] += 1\n",
    "            method_performance[\"SAT\"][\"times\"].append(time_sat)\n",
    "            print(f\"Exception: {str(e)[:30]}\", end=\"\")\n",
    "        validation_sat = validate_result(instance, td_sat, timeout_sat)\n",
    "        method_validations[\"SAT\"].append(validation_sat)\n",
    "        if validation_sat[\"validation_status\"] != \"no_golden_standard\":\n",
    "            if validation_sat[\"is_correct\"]:\n",
    "                print(\"OK\")\n",
    "            else:\n",
    "                if validation_sat[\"error_type\"] == \"timeout_not_optimal\":\n",
    "                    print(\"(timeout)\")\n",
    "                else:\n",
    "                    print(f\"({validation_sat['error_type']})\")\n",
    "        else:\n",
    "            print(\"no\")\n",
    "        consistency_check = \"unknown\"\n",
    "        if (td_ilp is not None and td_sat is not None and \n",
    "            status_ilp in [\"optimal\", \"timeout\"] and status_sat in [\"optimal\", \"timeout\"]):\n",
    "            consistency_check = \"consistent\" if td_ilp == td_sat else \"inconsistent\"\n",
    "            if consistency_check == \"inconsistent\":\n",
    "                print(f\"Inconsistent results: ILP={td_ilp}, SAT={td_sat}\")\n",
    "        base_record = {\n",
    "            \"experiment\": \"E4_6_ilp_vs_sat_unified\",\n",
    "            \"instance\": instance,\n",
    "            \"dataset\": dataset,\n",
    "            \"size_category\": size_category,\n",
    "            \"n\": graph_stats[\"n\"],\n",
    "            \"m\": graph_stats[\"m\"],\n",
    "            \"graph_density\": round(graph_stats[\"density\"], 4),\n",
    "            \"graph_components\": graph_stats[\"components\"],\n",
    "            \"consistency_check\": consistency_check,\n",
    "            \"timeout_used\": config['timeout_per_instance'],\n",
    "            \"max_nodes_limit\": config['max_nodes'],\n",
    "            \"max_edges_limit\": config['max_edges'],\n",
    "            \"golden_standard\": validation_ilp[\"golden_value\"],\n",
    "        }\n",
    "        ilp_record = base_record.copy()\n",
    "        ilp_record.update({\n",
    "            \"method\": \"ILP\",\n",
    "            \"treedepth\": td_ilp,\n",
    "            \"time\": round(time_ilp, 3),\n",
    "            \"status\": status_ilp,\n",
    "            \"timeout\": timeout_ilp,\n",
    "            \"preprocessing_enabled\": optimal_ilp_config['preprocessing_enabled'],\n",
    "            \"encoding_variant\": optimal_ilp_config['encoding_variant'],\n",
    "            \"mipstart_enabled\": optimal_ilp_config['mipstart_enabled'],\n",
    "            \"lazy_constraints_enabled\": optimal_ilp_config['lazy_constraints_enabled'],\n",
    "            \"gurobi_presolve\": optimal_ilp_config['gurobi_presolve'],\n",
    "            \"problem_variables\": stats_ilp.get(\"total_variables\", 0),\n",
    "            \"problem_constraints\": stats_ilp.get(\"total_constraints\", 0),\n",
    "            \"gurobi_objval\": td_ilp,\n",
    "            \"gurobi_mip_gap\": 0.0,\n",
    "            \"gurobi_node_count\": 0,\n",
    "            \"preprocessing_reduction\": stats_ilp.get(\"reduction_rate\", 0.0),\n",
    "            \"memory_usage_mb\": 0.0,\n",
    "            \"is_correct\": validation_ilp[\"is_correct\"],\n",
    "            \"validation_status\": validation_ilp[\"validation_status\"],\n",
    "            \"error_type\": validation_ilp[\"error_type\"]\n",
    "        })\n",
    "        results.append(ilp_record)\n",
    "        sat_record = base_record.copy()\n",
    "        sat_record.update({\n",
    "            \"method\": \"SAT\",\n",
    "            \"treedepth\": td_sat,\n",
    "            \"time\": round(time_sat, 3),\n",
    "            \"status\": status_sat,\n",
    "            \"timeout\": timeout_sat,\n",
    "            \"preprocessing_enabled\": optimal_ilp_config['preprocessing_enabled'],\n",
    "            \"encoding_variant\": \"sat_original_paper\",\n",
    "            \"mipstart_enabled\": False,\n",
    "            \"lazy_constraints_enabled\": False,\n",
    "            \"gurobi_presolve\": 0,\n",
    "            \"problem_variables\": stats_sat.get(\"sat_variables\", 0),\n",
    "            \"problem_constraints\": stats_sat.get(\"sat_clauses\", 0),\n",
    "            \"gurobi_objval\": td_sat,\n",
    "            \"gurobi_mip_gap\": 0.0,\n",
    "            \"gurobi_node_count\": 0,\n",
    "            \"preprocessing_reduction\": stats_sat.get(\"reduction_rate\", 0.0),\n",
    "            \"memory_usage_mb\": 0.0,\n",
    "            \"is_correct\": validation_sat[\"is_correct\"],\n",
    "            \"validation_status\": validation_sat[\"validation_status\"],\n",
    "            \"error_type\": validation_sat[\"error_type\"]\n",
    "        })\n",
    "        results.append(sat_record)\n",
    "        gc.collect()\n",
    "        if config[\"enable_progress_display\"] and (inst_idx % 3 == 0 or inst_idx == total_selected):\n",
    "            ilp_success = method_performance[\"ILP\"][\"optimal\"] + method_performance[\"ILP\"][\"timeout\"]\n",
    "            sat_success = method_performance[\"SAT\"][\"optimal\"] + method_performance[\"SAT\"][\"timeout\"]\n",
    "            ilp_analysis = analyze_validation_results(method_validations[\"ILP\"])\n",
    "            sat_analysis = analyze_validation_results(method_validations[\"SAT\"])\n",
    "            print(\"Progress summary:\")\n",
    "            print(f\"      Completed: {inst_idx}/{total_selected} ({inst_idx/total_selected*100:.1f}%)\")\n",
    "            print(f\"      ILP: Success {ilp_success}/{inst_idx} ({ilp_success/inst_idx*100:.1f}%), Accuracy {ilp_analysis.get('accuracy_rate', 0):.1%}\")\n",
    "            print(f\"      SAT: Success {sat_success}/{inst_idx} ({sat_success/inst_idx*100:.1f}%), Accuracy {sat_analysis.get('accuracy_rate', 0):.1%}\")\n",
    "            print(f\"      Total elapsed: {elapsed_time/60:.1f} min\")\n",
    "    total_experiment_time = time.time() - experiment_start_time\n",
    "    print(f\"\\nTotal experiment runtime: {total_experiment_time/3600:.2f} hours\")\n",
    "    output_file = OUTPUT_FILES[\"final_comparison\"]\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Experiment 4.6 - Optimal ILP vs SAT performance comparison (unified parameters)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Experiment configuration summary:\")\n",
    "    print(f\"  Number of test instances: {total_selected}\")\n",
    "    print(f\"  Unified time limit: {config['timeout_per_instance']}s ({config['timeout_per_instance']/60:.1f}min)\")\n",
    "    print(f\"  Node limit: {config['min_nodes']}-{config['max_nodes']}\")\n",
    "    print(f\"  Edge limit: ≤{config['max_edges']}\")\n",
    "    print(f\"  Total runtime: {total_experiment_time/3600:.2f} hours\")\n",
    "    ilp_results = [r for r in results if r[\"method\"] == \"ILP\"]\n",
    "    sat_results = [r for r in results if r[\"method\"] == \"SAT\"]\n",
    "    ilp_metrics = calculate_method_metrics(ilp_results, \"ILP\")\n",
    "    sat_metrics = calculate_method_metrics(sat_results, \"SAT\")\n",
    "    print(f\"\\n Accuracy and performance comparison (unified time limit {config['timeout_per_instance']}s):\")\n",
    "    print(f\"{'Method':<8} {'Accuracy':<10} {'Optimal':<10} {'Success':<10} {'Correct/Test':<12} {'Avg time':<12} {'Grade':<10}\")\n",
    "    print(\"-\" * 90)\n",
    "    for metrics in [ilp_metrics, sat_metrics]:\n",
    "        name = metrics[\"method_name\"]\n",
    "        accuracy = metrics[\"accuracy_rate\"]\n",
    "        optimal_rate = metrics.get(\"optimal_rate\", 0)\n",
    "        success_rate = metrics[\"success_rate\"]\n",
    "        correct = metrics[\"correct_instances\"]\n",
    "        testable = metrics[\"testable_instances\"]\n",
    "        avg_time = metrics[\"avg_time\"]\n",
    "        time_str = f\"{avg_time:.2f}s\" if avg_time != float('inf') else \"∞\"\n",
    "        if accuracy >= 0.95:\n",
    "            quality = \"A+\"\n",
    "        elif accuracy >= 0.90:\n",
    "            quality = \"A\"\n",
    "        elif accuracy >= 0.80:\n",
    "            quality = \"B\"\n",
    "        elif accuracy >= 0.70:\n",
    "            quality = \"C\"\n",
    "        elif accuracy >= 0.50:\n",
    "            quality = \"D\"\n",
    "        else:\n",
    "            quality = \"F\"\n",
    "        print(f\"{name:<8} {accuracy:>8.1%} {optimal_rate:>8.1%} {success_rate:>8.1%} {correct:>4}/{testable:<6} {time_str:>10} {quality:>8}\")\n",
    "    print(\"\\n Method selection (based on accuracy):\")\n",
    "    accuracy_diff = ilp_metrics[\"accuracy_rate\"] - sat_metrics[\"accuracy_rate\"]\n",
    "    if abs(accuracy_diff) < 0.02:\n",
    "        print(f\"  Accuracies are close (difference {abs(accuracy_diff):.1%} < 2%)\")\n",
    "        optimal_diff = ilp_metrics.get(\"optimal_rate\", 0) - sat_metrics.get(\"optimal_rate\", 0)\n",
    "        if abs(optimal_diff) > 0.05:\n",
    "            better_method = \"ILP\" if optimal_diff > 0 else \"SAT\"\n",
    "            print(f\"  Choose {better_method} (higher optimal rate: {optimal_diff:+.1%})\")\n",
    "            winner = better_method\n",
    "            reason = f\"Similar accuracy; {better_method} has higher optimal rate\"\n",
    "        else:\n",
    "            time_diff = sat_metrics[\"avg_time\"] - ilp_metrics[\"avg_time\"]\n",
    "            if abs(time_diff) > 10:\n",
    "                better_method = \"ILP\" if time_diff > 0 else \"SAT\"\n",
    "                print(f\"  Choose {better_method} (better time performance)\")\n",
    "                winner = better_method\n",
    "                reason = \"Similar accuracy and optimal rate; better time performance\"\n",
    "            else:\n",
    "                winner = \"ILP\"\n",
    "                reason = \"Similar across metrics; choose the more mature ILP method\"\n",
    "                print(\"  Choose ILP (metrics similar; choose the more mature method)\")\n",
    "    else:\n",
    "        winner = \"ILP\" if accuracy_diff > 0 else \"SAT\"\n",
    "        reason = f\"Higher accuracy for {winner} ({accuracy_diff:+.1%})\"\n",
    "        print(f\"  Choose {winner} (higher accuracy: {accuracy_diff:+.1%})\")\n",
    "    print(\"\\n Detailed performance analysis:\")\n",
    "    print(f\"\\nTiming statistics (unified limit {config['timeout_per_instance']}s):\")\n",
    "    ilp_times = [r[\"time\"] for r in ilp_results if r[\"status\"] in [\"optimal\", \"timeout\"]]\n",
    "    sat_times = [r[\"time\"] for r in sat_results if r[\"status\"] in [\"optimal\", \"timeout\"]]\n",
    "    if ilp_times and sat_times:\n",
    "        print(f\"  ILP: mean {np.mean(ilp_times):.2f}s, median {np.median(ilp_times):.2f}s, max {np.max(ilp_times):.2f}s\")\n",
    "        print(f\"  SAT: mean {np.mean(sat_times):.2f}s, median {np.median(sat_times):.2f}s, max {np.max(sat_times):.2f}s\")\n",
    "        time_ranges = [(0, 10), (10, 60), (60, 300), (300, config['timeout_per_instance'])]\n",
    "        print(\"  Time distribution:\")\n",
    "        for start, end in time_ranges:\n",
    "            ilp_count = len([t for t in ilp_times if start <= t < end])\n",
    "            sat_count = len([t for t in sat_times if start <= t < end])\n",
    "            range_desc = f\"{start}-{end}s\" if end < config['timeout_per_instance'] else f\">{start}s\"\n",
    "            print(f\"    {range_desc}: ILP {ilp_count}/{len(ilp_times)} ({ilp_count/len(ilp_times)*100:.1f}%), \"\n",
    "                    f\"SAT {sat_count}/{len(sat_times)} ({sat_count/len(sat_times)*100:.1f}%)\")\n",
    "    print(\"\\nBy-size analysis:\")\n",
    "    for category in [\"TINY\", \"SMALL\", \"MEDIUM\", \"LARGE\"]:\n",
    "        if category in size_categories and size_categories[category]:\n",
    "            instances_in_category = [r for r in results if r[\"size_category\"] == category]\n",
    "            if not instances_in_category:\n",
    "                continue\n",
    "            ilp_cat = [r for r in instances_in_category if r[\"method\"] == \"ILP\"]\n",
    "            sat_cat = [r for r in instances_in_category if r[\"method\"] == \"SAT\"]\n",
    "            if ilp_cat and sat_cat:\n",
    "                ilp_cat_metrics = calculate_method_metrics(ilp_cat, f\"ILP-{category}\")\n",
    "                sat_cat_metrics = calculate_method_metrics(sat_cat, f\"SAT-{category}\")\n",
    "                size_range = {\"TINY\": \"≤10\", \"SMALL\": \"11-20\", \"MEDIUM\": \"21-30\", \"LARGE\": \"31-50\"}[category]\n",
    "                print(f\"  {category} ({size_range} nodes, {len(ilp_cat)} instances):\")\n",
    "                print(f\"    ILP: accuracy {ilp_cat_metrics['accuracy_rate']:.1%}, optimal {ilp_cat_metrics.get('optimal_rate', 0):.1%}\")\n",
    "                print(f\"    SAT: accuracy {sat_cat_metrics['accuracy_rate']:.1%}, optimal {sat_cat_metrics.get('optimal_rate', 0):.1%}\")\n",
    "    min_accuracy = min(ilp_metrics[\"accuracy_rate\"], sat_metrics[\"accuracy_rate\"])\n",
    "    if min_accuracy < 0.5:\n",
    "        print(f\"\\n Critical warning: best method accuracy is only {min_accuracy:.1%}\")\n",
    "        print(\"This suggests potential implementation issues, but the experiment continues\")\n",
    "    elif min_accuracy >= 0.95:\n",
    "        print(\"\\n Excellent: both methods have accuracy ≥95%\")\n",
    "    elif min_accuracy >= 0.8:\n",
    "        print(\"\\n✓ Good: both methods have accuracy ≥80%\")\n",
    "    print(\"\\nConsistency analysis:\")\n",
    "    consistency_stats = {}\n",
    "    for r in results[::2]:\n",
    "        status = r[\"consistency_check\"]\n",
    "        consistency_stats[status] = consistency_stats.get(status, 0) + 1\n",
    "    for status, count in consistency_stats.items():\n",
    "        print(f\"  {status}: {count} instances\")\n",
    "    print(\"\\n Final recommendation:\")\n",
    "    print(f\"  Recommended method: {winner}\")\n",
    "    print(f\"  Reason: {reason}\")\n",
    "    print(f\"  Method quality: {winner} accuracy is {(ilp_metrics if winner == 'ILP' else sat_metrics)['accuracy_rate']:.1%}\")\n",
    "    global FINAL_OPTIMAL_CONFIG\n",
    "    FINAL_OPTIMAL_CONFIG = {\n",
    "        **optimal_ilp_config,\n",
    "        \"recommended_method\": winner,\n",
    "        \"recommendation_reason\": reason,\n",
    "        \"ilp_accuracy\": ilp_metrics[\"accuracy_rate\"],\n",
    "        \"sat_accuracy\": sat_metrics[\"accuracy_rate\"],\n",
    "        \"ilp_optimal_rate\": ilp_metrics.get(\"optimal_rate\", 0),\n",
    "        \"sat_optimal_rate\": sat_metrics.get(\"optimal_rate\", 0),\n",
    "        \"ilp_success_rate\": ilp_metrics[\"success_rate\"],\n",
    "        \"sat_success_rate\": sat_metrics[\"success_rate\"],\n",
    "        \"ilp_avg_time\": ilp_metrics[\"avg_time\"],\n",
    "        \"sat_avg_time\": sat_metrics[\"avg_time\"],\n",
    "        \"instances_tested\": total_selected,\n",
    "        \"experiment_duration_hours\": total_experiment_time / 3600,\n",
    "        \"unified_timeout\": config['timeout_per_instance'],\n",
    "        \"experiment_config\": config.copy()\n",
    "    }\n",
    "    print(f\"\\nResults saved to: {output_file}\")\n",
    "    print(\" Final method recommendation complete\")\n",
    "    return results, {\"ILP\": ilp_metrics, \"SAT\": sat_metrics}\n",
    "\n",
    "print(\" Notes on parameter tweaks:\")\n",
    "print(\"  To modify parameters, edit the EXPERIMENT_4_6_CONFIG dictionary above\")\n",
    "print(\"  Key parameters:\")\n",
    "print(\"    timeout_per_instance: time limit per instance (seconds)\")\n",
    "print(\"    max_nodes: maximum number of nodes\")\n",
    "print(\"    max_edges: maximum number of edges\")\n",
    "print(\"    skip_large_instances: whether to skip large instances\")\n",
    "print()\n",
    "\n",
    "print(\"Running Experiment 4.6 with unified configuration (accuracy first)...\")\n",
    "\n",
    "final_comparison_results, final_methods_comparison = run_ilp_vs_sat_experiment_with_unified_config()\n",
    "\n",
    "if final_comparison_results:\n",
    "    print(\"\\n Experiment 4.6 complete!\")\n",
    "    print(f\" Recommended method: {FINAL_OPTIMAL_CONFIG['recommended_method']}\")\n",
    "    print(f\" Recommended accuracy: ILP {FINAL_OPTIMAL_CONFIG['ilp_accuracy']:.1%}, \"\n",
    "          f\"SAT {FINAL_OPTIMAL_CONFIG['sat_accuracy']:.1%}\")\n",
    "    print(f\" Unified time limit: {FINAL_OPTIMAL_CONFIG['unified_timeout']}s\")\n",
    "    print(f\" Experiment duration: {FINAL_OPTIMAL_CONFIG['experiment_duration_hours']:.2f} hours\")\n",
    "    print(\"All experiments complete!\")\n",
    "else:\n",
    "    print(\"\\n Experiment 4.6 failed or was skipped\")\n",
    "\n",
    "print(\"\\n Parameter tweak examples:\")\n",
    "print(\"EXPERIMENT_4_6_CONFIG['timeout_per_instance'] = 120  # 2 minutes\")\n",
    "print(\"EXPERIMENT_4_6_CONFIG['max_nodes'] = 20\")\n",
    "print()\n",
    "print(\"EXPERIMENT_4_6_CONFIG['timeout_per_instance'] = 1200  # 20 minutes\")\n",
    "print(\"EXPERIMENT_4_6_CONFIG['max_nodes'] = 50\")\n",
    "print(\"EXPERIMENT_4_6_CONFIG['skip_large_instances'] = False\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
