{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdecfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3 \n",
    "# coding:utf-8\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "import argparse\n",
    "import subprocess\n",
    "import importlib\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "\n",
    "for pkg in (\"networkx\", \"gurobipy\"):\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "\n",
    "import networkx as nx\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "SKIP_LIST = {\"Paley17\"}\n",
    "DEFAULT_MAX_V = 20\n",
    "DEFAULT_MAX_E = 2000\n",
    "DEFAULT_TIME = 180\n",
    "DEFAULT_OUTFILE = \"results_treedepth_ilp_optimized99.csv\"\n",
    "DEFAULT_THREADS = max(1, os.cpu_count() // 4)\n",
    "\n",
    "\n",
    "def apex_vertices(g):\n",
    "    \"\"\"\n",
    "    Remove apex vertices (degree == n-1), return (reduced_graph, count_removed).\n",
    "    \"\"\"\n",
    "    if g.number_of_nodes() <= 1:\n",
    "        return g, 0\n",
    "    \n",
    "    to_remove = [u for u, d in g.degree() if d == g.number_of_nodes() - 1]\n",
    "    g.remove_nodes_from(to_remove)\n",
    "    buff = len(to_remove)\n",
    "    g = nx.convert_node_labels_to_integers(g, first_label=0, ordering=\"default\")\n",
    "    return g, buff\n",
    "\n",
    "\n",
    "def degree_one_reduction(g):\n",
    "    \"\"\"\n",
    "    Remove all but one degree-1 neighbor per vertex.\n",
    "    \"\"\"\n",
    "    if g.number_of_nodes() <= 1:\n",
    "        return g\n",
    "    \n",
    "    to_remove = set()\n",
    "    for u in g.nodes():\n",
    "        seen = False\n",
    "        for v in list(g.neighbors(u)):\n",
    "            if g.degree(v) == 1:\n",
    "                if not seen:\n",
    "                    seen = True\n",
    "                else:\n",
    "                    to_remove.add(v)\n",
    "    \n",
    "    g.remove_nodes_from(to_remove)\n",
    "    return nx.convert_node_labels_to_integers(g, first_label=0, ordering=\"default\")\n",
    "\n",
    "\n",
    "def preprocess_graph(G, enable_preprocessing=True):\n",
    "    stats = {\n",
    "        \"original_nodes\": G.number_of_nodes(),\n",
    "        \"original_edges\": G.number_of_edges(),\n",
    "        \"degree_one_removed\": 0,\n",
    "        \"apex_removed\": 0,\n",
    "        \"final_nodes\": 0,\n",
    "        \"final_edges\": 0\n",
    "    }\n",
    "    \n",
    "    if not enable_preprocessing:\n",
    "        stats[\"final_nodes\"] = G.number_of_nodes()\n",
    "        stats[\"final_edges\"] = G.number_of_edges()\n",
    "        return G.copy(), 0, stats\n",
    "    \n",
    "    g1 = degree_one_reduction(G.copy())\n",
    "    stats[\"degree_one_removed\"] = stats[\"original_nodes\"] - g1.number_of_nodes()\n",
    "    \n",
    "    g2, apex_buffer = apex_vertices(g1)\n",
    "    stats[\"apex_removed\"] = apex_buffer\n",
    "    stats[\"final_nodes\"] = g2.number_of_nodes()\n",
    "    stats[\"final_edges\"] = g2.number_of_edges()\n",
    "    \n",
    "    return g2, apex_buffer, stats\n",
    "\n",
    "\n",
    "def read_edge_file(filename: str) -> Tuple[List[Tuple[int, int]], int]:\n",
    "    edges = []\n",
    "    n_declared = 0\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('c'):\n",
    "                continue\n",
    "            if line.startswith('p '):\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 3:\n",
    "                    n_declared = int(parts[2])\n",
    "            elif line.startswith('e '):\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 3:\n",
    "                    u, v = int(parts[1]) - 1, int(parts[2]) - 1\n",
    "                    edges.append((u, v))\n",
    "    return edges, n_declared\n",
    "\n",
    "\n",
    "def create_graph_from_edges(edges: List[Tuple[int, int]], n_nodes: int) -> nx.Graph:\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(n_nodes))\n",
    "    G.add_edges_from(edges)\n",
    "    return G\n",
    "\n",
    "\n",
    "def estimate_bounds(G: nx.Graph) -> Tuple[int, int]:\n",
    "    n = G.number_of_nodes()\n",
    "    if n <= 1:\n",
    "        return n, n\n",
    "\n",
    "    def dfs_height(component_nodes: List[int]) -> int:\n",
    "        visited = set()\n",
    "        maxh = 1\n",
    "        for start in component_nodes:\n",
    "            if start in visited:\n",
    "                continue\n",
    "            stack = [(start, 1, None)]\n",
    "            while stack:\n",
    "                v, d, parent = stack.pop()\n",
    "                if v in visited:\n",
    "                    continue\n",
    "                visited.add(v)\n",
    "                maxh = max(maxh, d)\n",
    "                for w in G.neighbors(v):\n",
    "                    if w != parent:\n",
    "                        stack.append((w, d + 1, v))\n",
    "        return maxh\n",
    "\n",
    "    U = 0\n",
    "    for comp in nx.connected_components(G):\n",
    "        U = max(U, dfs_height(list(comp)))\n",
    "    U = max(U, 1)\n",
    "\n",
    "    LB = 1\n",
    "    for comp in nx.connected_components(G):\n",
    "        H = G.subgraph(comp)\n",
    "        if H.number_of_nodes() == 1:\n",
    "            LBc = 1\n",
    "        else:\n",
    "            src = next(iter(comp))\n",
    "            dist1 = nx.single_source_shortest_path_length(G, src)\n",
    "            far = max(dist1, key=dist1.get)\n",
    "            dist2 = nx.single_source_shortest_path_length(G, far)\n",
    "            diam = max(dist2.values())\n",
    "            LBc = max(1, math.ceil(math.log2(diam + 1)))\n",
    "        LB = max(LB, LBc)\n",
    "\n",
    "    U = min(U, n)\n",
    "    return LB, U\n",
    "\n",
    "\n",
    "def build_dfs_mipstart(G: nx.Graph):\n",
    "    nodes = list(G.nodes())\n",
    "    parent = {v: None for v in nodes}\n",
    "    depth = {v: None for v in nodes}\n",
    "\n",
    "    def dfs(root: int):\n",
    "        stack = [(root, 1, None)]\n",
    "        while stack:\n",
    "            v, d, p = stack.pop()\n",
    "            if depth[v] is not None:\n",
    "                continue\n",
    "            depth[v] = d\n",
    "            parent[v] = p\n",
    "            for w in G.neighbors(v):\n",
    "                if depth[w] is None:\n",
    "                    stack.append((w, d + 1, v))\n",
    "\n",
    "    for comp in nx.connected_components(G):\n",
    "        r = min(comp)\n",
    "        dfs(r)\n",
    "\n",
    "    H = max(depth.values()) if depth else 0\n",
    "\n",
    "    def is_ancestor(u, v):\n",
    "        cur = parent[v]\n",
    "        while cur is not None:\n",
    "            if cur == u:\n",
    "                return True\n",
    "            cur = parent[cur]\n",
    "        return False\n",
    "\n",
    "    ancestor = {(u, v): int(u != v and is_ancestor(u, v)) for u in nodes for v in nodes}\n",
    "    root = {v: int(parent[v] is None) for v in nodes}\n",
    "\n",
    "    return {\"H\": H, \"depth\": depth, \"parent\": parent, \"ancestor\": ancestor, \"root\": root}\n",
    "\n",
    "\n",
    "def lazy_transitivity_callback(model: gp.Model, where):\n",
    "    if where != GRB.Callback.MIPSOL:\n",
    "        return\n",
    "    a = model._a\n",
    "    nodes = model._nodes\n",
    "    aval = {(i, j): model.cbGetSolution(a[i, j]) for i in nodes for j in nodes if i != j}\n",
    "    for i in nodes:\n",
    "        for j in nodes:\n",
    "            if i == j or aval[(i, j)] < 0.5:\n",
    "                continue\n",
    "            for k in nodes:\n",
    "                if k != i and k != j and aval[(j, k)] > 0.5 and aval[(i, k)] < 0.5:\n",
    "                    model.cbLazy(a[i, k] >= a[i, j] + a[j, k] - 1)\n",
    "\n",
    "\n",
    "def solve_single_component(G: nx.Graph, time_limit: int, threads: int) -> Tuple[Optional[int], bool]:\n",
    "    n = G.number_of_nodes()\n",
    "    if n <= 1:\n",
    "        return n, False\n",
    "\n",
    "    nodes = list(G.nodes())\n",
    "    edges = list(G.edges())\n",
    "\n",
    "    LB, UB = estimate_bounds(G)\n",
    "    U = UB if UB > 0 else n\n",
    "\n",
    "    model = gp.Model(\"Treedepth_ILP_Direct\")\n",
    "    model.Params.OutputFlag = 0\n",
    "    model.Params.TimeLimit = time_limit\n",
    "    model.Params.Threads = threads\n",
    "    model.Params.MIPFocus = 1\n",
    "    model.Params.Heuristics = 0.4\n",
    "    model.Params.Presolve = 2\n",
    "    model.Params.Cuts = 2\n",
    "    model.Params.Symmetry = 2\n",
    "    model.Params.NodefileStart = 0.5\n",
    "    model.Params.LazyConstraints = 1\n",
    "\n",
    "    d = model.addVars(nodes, vtype=GRB.INTEGER, lb=1, ub=U, name=\"d\")\n",
    "    r = model.addVars(nodes, vtype=GRB.BINARY, name=\"r\")\n",
    "    p = model.addVars(nodes, nodes, vtype=GRB.BINARY, name=\"p\")\n",
    "    a = model.addVars(nodes, nodes, vtype=GRB.BINARY, name=\"a\")\n",
    "\n",
    "    try:\n",
    "        model.setObjective(gp.max_([d[v] for v in nodes]), GRB.MINIMIZE)\n",
    "        use_direct_max = True\n",
    "    except:\n",
    "        max_depth = model.addVar(vtype=GRB.INTEGER, lb=LB, ub=UB, name=\"max_depth\")\n",
    "        model.setObjective(max_depth, GRB.MINIMIZE)\n",
    "        for v in nodes:\n",
    "            model.addConstr(max_depth >= d[v])\n",
    "        use_direct_max = False\n",
    "\n",
    "    for i in nodes:\n",
    "        model.addConstr(p[i, i] == 0)\n",
    "        model.addConstr(a[i, i] == 0)\n",
    "\n",
    "    for v in nodes:\n",
    "        model.addConstr(gp.quicksum(p[u, v] for u in nodes if u != v) + r[v] == 1)\n",
    "        model.addConstr(d[v] <= 1 + U * (1 - r[v]))\n",
    "        model.addConstr(d[v] >= 1 - U * (1 - r[v]))\n",
    "\n",
    "    for u in nodes:\n",
    "        for v in nodes:\n",
    "            if u != v:\n",
    "                model.addConstr(d[v] - d[u] >= 1 - U * (1 - p[u, v]))\n",
    "                model.addConstr(d[v] - d[u] <= 1 + U * (1 - p[u, v]))\n",
    "\n",
    "    for u in nodes:\n",
    "        for v in nodes:\n",
    "            if u != v:\n",
    "                model.addConstr(d[u] + 1 <= d[v] + U * (1 - a[u, v]))\n",
    "\n",
    "    for v in nodes:\n",
    "        model.addConstr(gp.quicksum(a[u, v] for u in nodes if u != v) == d[v] - 1)\n",
    "\n",
    "    for u in nodes:\n",
    "        for v in nodes:\n",
    "            if u != v:\n",
    "                model.addConstr(a[u, v] >= p[u, v])\n",
    "\n",
    "    for i in nodes:\n",
    "        for j in nodes:\n",
    "            if i != j:\n",
    "                model.addConstr(a[i, j] + a[j, i] <= 1)\n",
    "\n",
    "    for u, v in edges:\n",
    "        model.addConstr(a[u, v] + a[v, u] >= 1)\n",
    "\n",
    "    ms = build_dfs_mipstart(G)\n",
    "    if not use_direct_max:\n",
    "        max_depth.Start = ms[\"H\"]\n",
    "    for v in nodes:\n",
    "        d[v].Start = ms[\"depth\"][v]\n",
    "        r[v].Start = ms[\"root\"][v]\n",
    "        for u in nodes:\n",
    "            p[u, v].Start = 1 if ms[\"parent\"][v] == u else 0\n",
    "            a[u, v].Start = ms[\"ancestor\"][(u, v)]\n",
    "\n",
    "    model._a = a\n",
    "    model._nodes = nodes\n",
    "    model.optimize(lambda m, w: lazy_transitivity_callback(m, w))\n",
    "\n",
    "    if model.Status == GRB.OPTIMAL:\n",
    "        result = max(int(d[v].X) for v in nodes)\n",
    "        return result, False\n",
    "    elif model.Status == GRB.TIME_LIMIT and model.SolCount > 0:\n",
    "        result = max(int(d[v].X) for v in nodes)\n",
    "        return result, True\n",
    "    else:\n",
    "        return None, True\n",
    "\n",
    "\n",
    "def build_ilp_and_solve_direct(G: nx.Graph, time_limit: int, threads: int, enable_preprocessing: bool = True) -> Tuple[Optional[int], bool, Dict]:\n",
    "    processed_G, apex_buffer, preprocess_stats = preprocess_graph(G, enable_preprocessing)\n",
    "    \n",
    "    if processed_G.number_of_nodes() <= 1:\n",
    "        return processed_G.number_of_nodes() + apex_buffer, False, preprocess_stats\n",
    "\n",
    "    max_td = 0\n",
    "    overall_timeout = False\n",
    "    \n",
    "    for comp in nx.connected_components(processed_G):\n",
    "        sub_G = processed_G.subgraph(comp).copy()\n",
    "        sub_G = nx.convert_node_labels_to_integers(sub_G, first_label=0, ordering=\"default\")\n",
    "        \n",
    "        td, timeout = solve_single_component(sub_G, time_limit, threads)\n",
    "        if td is None:\n",
    "            return None, True, preprocess_stats\n",
    "        \n",
    "        max_td = max(max_td, td)\n",
    "        if timeout:\n",
    "            overall_timeout = True\n",
    "    \n",
    "    final_td = max_td + apex_buffer\n",
    "    return final_td, overall_timeout, preprocess_stats\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"ILP treedepth solver (with preprocessing)\")\n",
    "    parser.add_argument(\"--timeout\", type=int, default=DEFAULT_TIME)\n",
    "    parser.add_argument(\"--max_v\", type=int, default=DEFAULT_MAX_V)\n",
    "    parser.add_argument(\"--max_e\", type=int, default=DEFAULT_MAX_E)\n",
    "    parser.add_argument(\"--threads\", type=int, default=DEFAULT_THREADS)\n",
    "    parser.add_argument(\"--output\", type=str, default=DEFAULT_OUTFILE)\n",
    "    parser.add_argument(\"--enable_preprocessing\", action=\"store_true\", default=False,\n",
    "                       help=\"Enable graph preprocessing (degree-1 reduction and apex removal)\")\n",
    "    parser.add_argument(\"--disable_preprocessing\", action=\"store_true\", default=False,\n",
    "                       help=\"Disable graph preprocessing\")\n",
    "    parser.add_argument(\"files\", nargs=\"*\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    if args.disable_preprocessing:\n",
    "        enable_preprocessing = False\n",
    "    elif args.enable_preprocessing:\n",
    "        enable_preprocessing = True\n",
    "    else:\n",
    "        enable_preprocessing = True\n",
    "\n",
    "    if args.files:\n",
    "        file_patterns = args.files\n",
    "    else:\n",
    "        file_patterns = [\"inputs/famous/*.edge\", \"inputs/standard/*.edge\"]\n",
    "\n",
    "    files = []\n",
    "    for pat in file_patterns:\n",
    "        files.extend(glob.glob(pat))\n",
    "    files = sorted(set(files))\n",
    "    if not files:\n",
    "        print(\"No .edge files found\")\n",
    "        return\n",
    "\n",
    "    print(f\"Preprocessing: {'Enabled' if enable_preprocessing else 'Disabled'}\")\n",
    "    \n",
    "    stats = {\"solved\": 0, \"timeout\": 0, \"skipped\": 0, \"failed\": 0}\n",
    "    results_detail = []\n",
    "\n",
    "    csv_headers = [\n",
    "        \"dataset\", \"instance\", \"n_original\", \"m_original\", \n",
    "        \"n_after_preprocess\", \"m_after_preprocess\",\n",
    "        \"degree_one_removed\", \"apex_removed\",\n",
    "        \"treedepth\", \"timeout\", \"time_sec\"\n",
    "    ]\n",
    "\n",
    "    with open(args.output, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(csv_headers)\n",
    "\n",
    "        for idx, filepath in enumerate(files, 1):\n",
    "            dataset = os.path.basename(os.path.dirname(filepath))\n",
    "            instance = os.path.splitext(os.path.basename(filepath))[0]\n",
    "            print(f\"\\n[{idx}/{len(files)}] Processing {dataset}/{instance}\")\n",
    "\n",
    "            if instance in SKIP_LIST:\n",
    "                print(\"  Skipped (blacklist)\")\n",
    "                stats[\"skipped\"] += 1\n",
    "                results_detail.append((instance, \"-\", \"Skipped\"))\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                edges, n_declared = read_edge_file(filepath)\n",
    "                G = create_graph_from_edges(edges, n_declared)\n",
    "                n, m = G.number_of_nodes(), G.number_of_edges()\n",
    "                print(f\"  Original size: n={n}, m={m}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error: {e}\")\n",
    "                stats[\"failed\"] += 1\n",
    "                results_detail.append((instance, \"-\", \"Failed\"))\n",
    "                continue\n",
    "\n",
    "            if n > args.max_v or m > args.max_e:\n",
    "                print(f\"  Skipped (exceeds size limit: n>{args.max_v} or m>{args.max_e})\")\n",
    "                stats[\"skipped\"] += 1\n",
    "                results_detail.append((instance, \"-\", \"Skipped\"))\n",
    "                continue\n",
    "\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                td, is_timeout, preprocess_stats = build_ilp_and_solve_direct(\n",
    "                    G, args.timeout, args.threads, enable_preprocessing\n",
    "                )\n",
    "                solve_time = time.time() - start_time\n",
    "                \n",
    "                if enable_preprocessing:\n",
    "                    print(f\"  Preprocessing: degree-1 removed={preprocess_stats['degree_one_removed']}, \"\n",
    "                          f\"apex removed={preprocess_stats['apex_removed']}, \"\n",
    "                          f\"final size: n={preprocess_stats['final_nodes']}, m={preprocess_stats['final_edges']}\")\n",
    "                \n",
    "                if td is not None:\n",
    "                    if is_timeout:\n",
    "                        stats[\"timeout\"] += 1\n",
    "                        print(f\"  Result: treedepth≤{td} (timeout), time {solve_time:.2f}s\")\n",
    "                        results_detail.append((instance, f\"≤{td}\", \"Timeout\"))\n",
    "                    else:\n",
    "                        stats[\"solved\"] += 1\n",
    "                        print(f\"  Result: treedepth={td} (optimal), time {solve_time:.2f}s\")\n",
    "                        results_detail.append((instance, str(td), \"Optimal\"))\n",
    "                else:\n",
    "                    stats[\"failed\"] += 1\n",
    "                    print(f\"  Failed: could not solve, time {solve_time:.2f}s\")\n",
    "                    results_detail.append((instance, \"-\", \"Failed\"))\n",
    "                \n",
    "                writer.writerow([\n",
    "                    dataset, instance, n, m, \n",
    "                    preprocess_stats[\"final_nodes\"], preprocess_stats[\"final_edges\"],\n",
    "                    preprocess_stats[\"degree_one_removed\"], preprocess_stats[\"apex_removed\"],\n",
    "                    td, is_timeout, round(solve_time, 2)\n",
    "                ])\n",
    "            except Exception as e:\n",
    "                solve_time = time.time() - start_time\n",
    "                print(f\"  Error: {e}\")\n",
    "                stats[\"failed\"] += 1\n",
    "                results_detail.append((instance, \"-\", \"Failed\"))\n",
    "                writer.writerow([\n",
    "                    dataset, instance, n, m, 0, 0, 0, 0,\n",
    "                    None, True, round(solve_time, 2)\n",
    "                ])\n",
    "\n",
    "    print(\"\\n=== Processing complete ===\")\n",
    "    print(f\"Preprocessing: {'Enabled' if enable_preprocessing else 'Disabled'}\")\n",
    "    print(f\"Optimal solutions: {stats['solved']}\")\n",
    "    print(f\"Timeouts: {stats['timeout']}\")\n",
    "    print(f\"Skipped: {stats['skipped']}\")\n",
    "    print(f\"Failed: {stats['failed']}\")\n",
    "    print(f\"Results saved to: {args.output}\")\n",
    "\n",
    "    print(\"\\n=== Summary by category ===\")\n",
    "    def show_category(title, category):\n",
    "        items = [f\"{name}({td})\" for name, td, st in results_detail if st == category]\n",
    "        print(f\"[{title} {len(items)}]\")\n",
    "        if items:\n",
    "            print(\", \".join(items))\n",
    "\n",
    "    show_category(\"Optimal\", \"Optimal\")\n",
    "    show_category(\"Timeout\", \"Timeout\")\n",
    "    show_category(\"Failed\", \"Failed\")\n",
    "    show_category(\"Skipped\", \"Skipped\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
